{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from skimage import exposure\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import parser\n",
    "import csv\n",
    "\n",
    "import itertools\n",
    "\n",
    "import h5py\n",
    "import os, fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fracdim part based on https://stackoverflow.com/questions/44793221/python-fractal-box-count-fractal-dimension\n",
    "Not returning skewdness or kurtosis here but user can add these in \n",
    "and adjust corresponding parts of code further on in the rest of the notebook ###\n",
    "'''\n",
    "\n",
    "def stats_mbfracdim(data, threshold, switch):\n",
    "    \n",
    "    # 7 stats part\n",
    "    min_data = np.nanmin(data)\n",
    "    max_data = np.nanmax(data)\n",
    "    mean_data = np.nanmean(data)\n",
    "    median_data = np.nanmedian(data)\n",
    "    std_data = np.nanstd(data)\n",
    "    if len(np.where(np.array(data).flatten() != np.array(data).flatten())[0]) > 0:\n",
    "        skew_data = float(skew(data.flatten(),nan_policy='omit').data)\n",
    "    else:\n",
    "        skew_data = skew(data.flatten(),nan_policy='omit')\n",
    "    kurtosis_data = kurtosis(data.flatten(),nan_policy='omit')\n",
    "    \n",
    "    # fracdim part: Only for 2d image\n",
    "    assert(len(data.shape) == 2)\n",
    "\n",
    "    ind_nan = np.where(np.array(data.flatten()) != np.array(data.flatten()))[0]\n",
    "    data.flat[ind_nan] = 0.\n",
    "\n",
    "    def boxcount(data, k):\n",
    "        S = np.add.reduceat(\n",
    "                np.add.reduceat(data, np.arange(0, data.shape[0], k), axis=0),\n",
    "                       np.arange(0, data.shape[1], k), axis=1)\n",
    "\n",
    "        # We count non-empty (0) and non-full boxes (k*k)\n",
    "        return len(np.where((S > 0) & (S < k*k))[0])\n",
    "\n",
    "    # Transform data into a binary array\n",
    "    if switch == 1:\n",
    "        data = (data < threshold)\n",
    "    elif switch == -1:\n",
    "        data = (data > threshold)\n",
    "\n",
    "    # Minimal dimension of image\n",
    "    p = np.min(data.shape)\n",
    "\n",
    "    # Greatest power of 2 less than or equal to p\n",
    "    n = 2**np.floor(np.log(p)/np.log(2))\n",
    "\n",
    "    # Extract the exponent\n",
    "    n = int(np.log(n)/np.log(2))\n",
    "\n",
    "    # Build successive box sizes (from 2**n down to 2**1)\n",
    "    sizes = 2**np.arange(n, 1, -1)\n",
    "\n",
    "    # Actual box counting with decreasing size\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        counts.append(boxcount(data, size))\n",
    "\n",
    "    # Fit the successive log(sizes) with log (counts)\n",
    "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
    "    \n",
    "    return np.array([max_data, min_data, mean_data, std_data, -coeffs[0], 0])\n",
    "    ### 0 is included as the last entry of the returned array in order to be able to exclude features too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIND SPECIFIED PATTERN USING FNMATCH \n",
    "\"\"\"\n",
    "def pattern_finder(home_dir, pattern):\n",
    "    \n",
    "    names = []\n",
    "    for root, dirs, files in os.walk(home_dir):\n",
    "        for name in files:\n",
    "            if fnmatch.fnmatch(name, pattern):\n",
    "                name = str(name)\n",
    "                names.append(name)\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the deep learning system investigated in this project is the forecast of the N-S component of the IMF, Bz, measured in the GSM coordinate system at Lagrange point 1 (L1), in a two day window, 3 - 5 days ahead of the corresponding Solar and Heliophysics Observatory (SOHO) image products.\n",
    "This challenge is cast as a binary classification problem and compared with a Gaussian Naive Bayes classifier baseline. \n",
    "The value computed is not the raw Bz from OMNI but rather we define the new Bz as the min of raw Bz minus the mean of raw Bz in the above mentioned two-day window.\n",
    "The ground truth labels are obtained from the low-resolution, hourly-averaged values are obtained from the **OMNI** database: [OMNI2](https://omniweb.gsfc.nasa.gov/html/ow_data.html). The time range from this OMNI set that has been made available here is from Jan. 1st 1999 till Dec. 31st 2010. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the path to the downloaded OMNI data set to the ```np.genfromtxt()``` function in the cell below. \n",
    "The following definition for the 'nulls' variable comes from Section 3 of '**_Description of records and words_**' from the 'Fill values' column. There are a total of 57 columns listed but only 55 columns are actually present since 'F9.6' and 'F7.4' (columns 55 and 56, respectively) are omitted in the data set. Descriptions of the data are provided at the above NASA url. This step is performed to homogenize the representations of all values that are missing in the data set. Although this step is not strictly needed for our use-case here, it is useful if the user wished to use a different variable as their ground truth label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_OMNI_DATA = '/home/carl/Documents/OMNI_DATA/'\n",
    "omni2_data_raw = np.genfromtxt(f'{PATH_TO_OMNI_DATA}omni2_1999-01-01_to_2010-12-31.txt')\n",
    "nulls = np.array([np.nan, np.nan, np.nan, 9999, 99, 99, 999, 999, 999.9, 999.9, 999.9, 999.9,\n",
    "    999.9, 999.9, 999.9, 999.9, 999.9, 999.9, 999.9, 999.9, 999.9, 999.9,\n",
    "    9999999., 999.9, 9999., 999.9, 999.9, 9.999, 99.99, 9999999., 999.9, 9999.,\n",
    "    999.9, 999.9, 9.999, 999.99, 999.99, 999.9, 99, 999, 99999, 9999,\n",
    "    999999.99, 99999.99, 99999.99, 99999.99, 99999.99, 99999.99, 0, 999, 999.9,\n",
    "    999.9, 99999, 99999, 99.9\n",
    "])\n",
    "print('len(nulls):', len(nulls))\n",
    "omni2_data_nan_processed = np.where(omni2_data_raw == nulls[None, :], np.nan, omni2_data_raw)\n",
    "print('shape(omni2_data_nan_processed):', np.shape(omni2_data_nan_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert OMNI date-time format of 'Year-Day_of_Year-Time_of_Day' to date-time object.\n",
    "\n",
    "Choose the Bz GSM column which is the 16th column (counting the pythonic way from 0)\n",
    "\n",
    "Identify dates for which the ground truth label is missing.\n",
    "\n",
    "**Note**: If there would be missing ground truth labels that would influence our approach, we would need to account for this by removing corresponding SOHO images from the data cubes. From the procedure used in this project to arrive at the computed Bz value, these missing raw Bz values do not require any modification to the pipeline produced image cubes in this case. In another ground truth label setting or forecasting window, they might.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni2_date = [datetime(int(yr),1,1) + timedelta(days = int(day) - 1) + timedelta(hours = hr) for yr,day,hr in np.vstack((omni2_data_nan_processed.T[0],omni2_data_nan_processed.T[1], omni2_data_nan_processed.T[2])).T ]\n",
    "\n",
    "omni2_Bz = omni2_data_nan_processed.T[16]\n",
    "\n",
    "missing_Bz_vals_ind = np.where(np.array(omni2_Bz) != np.array(omni2_Bz))[0]\n",
    "print('Missing ground truth labels from Jan. 1st 1999 to Dec. 31st 2010 in data:\\n', [str(omni2_date[ind]) for ind in missing_Bz_vals_ind])\n",
    "print('Number of missing ground truth labels:', len(missing_Bz_vals_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert above transformed OMNI date-time object to a string for efficient matching when comparing with synced string date-times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni2_date_copy_str = [str(elem) for elem in omni2_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the path to the synced times contained in the **CSV** file for which to obtain the ground truth labels and specify ```dtype = 'str'```. \n",
    "As a check, ensure that the output length corresponds to the length of the CSV file which you expect to have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Need to adjust input according to if have 1, 3, or 7 SOHO products\n",
    "'''\n",
    "\n",
    "path_to_synced_times = '/home/carl/Documents/synced_csvs_1_3_7fams/'\n",
    "synced_times_file_name = '1999-01-01_to_2010-12-31_EIT304_6_6_128_times_sync_All7.csv'\n",
    "#'1999-01-01_to_2010-12-31_MDI_96m_6_6_128_times_sync_onlyMDI.csv'\n",
    "#'1999-01-01_to_2010-12-31_MDI_96m_6_6_128_times_sync_3products.csv'\n",
    "synced_times = np.genfromtxt(f'{path_to_synced_times}{synced_times_file_name}',dtype = 'str')\n",
    "print('len(synced_times):', len(synced_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert '19990202160302' string date-time format to a date-time object and compute the value of the date-time object 3 days ahead of the time found after rounding to the nearest hour and convert to string. Rounding is done to enable direct comparison with the OMNI date-time objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synced_times_datetimes = [parser.parse(elem) for elem in synced_times]\n",
    "synced_times_datetimes_rounded = [elem.replace(second = 0, microsecond = 0, minute = 0, hour = elem.hour) + timedelta(hours = elem.minute//30) for elem in synced_times_datetimes]\n",
    "synced_times_datetimes_rounded_3days_ahead = np.array(synced_times_datetimes_rounded) + timedelta(days = 3)\n",
    "synced_times_datetimes_rounded_3days_ahead_copy_str = [str(elem) for elem in synced_times_datetimes_rounded_3days_ahead]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can compare synced times directly with the times from OMNI data and compute Bmin - Bmean, 3-5 days ahead of the SOHO image product times. The figure of 48 is added in order to arrive at the 5 day mark of the two-day window. Need to use np.nanmin and np.nanmean as a result of the 24 missing raw Bz values. This is the most time consuming step thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omni2_3days_ahead_ind = [np.where(np.array(omni2_date_copy_str) == elem)[0][0] for elem in synced_times_datetimes_rounded_3days_ahead_copy_str]\n",
    "omni2_Bzmin_minus_mean_3to5days_ahead = [ np.round(np.nanmin(omni2_Bz[elem: elem + 48]) - np.nanmean(omni2_Bz[elem: elem + 48]),2) for elem in omni2_3days_ahead_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option to output the computed Bz ground truth labels corresponding to the synced times to a new CSV file in the same directory as the synced times. The user can change the name for the CSV file that is to be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "User should provide a more descriptive name to include the types of synced products for the name of the CSV file\n",
    "or save the CSV file in a different directory\n",
    "\n",
    "Optional\n",
    "'''\n",
    "\n",
    "with open(f'{path_to_synced_times}Bz_min_minus_mean_3to5days_ahead.csv', 'a') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(omni2_Bzmin_minus_mean_3to5days_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Freeing the memory taken up by this variable\n",
    "'''\n",
    "omni2_data_nan_processed = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the 'rolling data split'. In first year of data: training is Feb. - Aug., validation is Oct. and testing is Nov. Each subsequent year these periods are cyclically moved forward by one month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building up the rolling indices scheme\n",
    "'''\n",
    "\n",
    "synced_times_datetimes_rounded_years = np.array([elem.year for elem in synced_times_datetimes_rounded])\n",
    "synced_times_datetimes_rounded_months = np.array([elem.month for elem in synced_times_datetimes_rounded])\n",
    "\n",
    "start_ind_train = []\n",
    "end_ind_train = []\n",
    "\n",
    "start_ind_valid = []\n",
    "end_ind_valid = []\n",
    "\n",
    "start_ind_test = []\n",
    "end_ind_test = []\n",
    "\n",
    "\n",
    "# Hardcoded: Feb [0] - Aug [6], Oct [8], Nov [9] in Python index for start of year in new_month_order sequence\n",
    "# In our case will have 18 indices because of splitting of indices to obtain proper start and end indices\n",
    "\n",
    "month_order = np.arange(1,12+1)\n",
    "\n",
    "counter = 1\n",
    "for yr in range(synced_times_datetimes_rounded_years[0],synced_times_datetimes_rounded_years[-1] + 1):\n",
    "    new_month_order = np.roll(month_order,-counter) #since we start from Feb.1999 so roll back year by 1 (i.e., -1) \n",
    "    \n",
    "    if new_month_order[6] < new_month_order[0]: # Aug. is [6] in the first new_month_order for 1999.\n",
    "        pos_one = np.where(new_month_order == 1)[0] # 1 is the location of Jan.\n",
    "        ind_start_train_part1 = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[pos_one]))[0][0]\n",
    "        ind_end_train_part1 = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[pos_one + (6-pos_one)]))[0][-1] #6 is for Aug. position\n",
    "        \n",
    "        ind_start_train_part2 = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[0]))[0][0]\n",
    "        ind_end_train_part2 = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[pos_one-1]))[0][-1]\n",
    "\n",
    "        start_ind_train.append(ind_start_train_part1) \n",
    "        start_ind_train.append(ind_start_train_part2)\n",
    "        end_ind_train.append(ind_end_train_part1)\n",
    "        end_ind_train.append(ind_end_train_part2)\n",
    "        \n",
    "        \n",
    "        ind_start_valid = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[8]))[0][0] \n",
    "        start_ind_valid.append(ind_start_valid)\n",
    "\n",
    "        ind_end_valid = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[8]))[0][-1]\n",
    "        end_ind_valid.append(ind_end_valid)\n",
    "\n",
    "\n",
    "        ind_start_test = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[9]))[0][0]\n",
    "        start_ind_test.append(ind_start_test)\n",
    "\n",
    "        ind_end_test = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[9]))[0][-1]\n",
    "        end_ind_test.append(ind_end_test) \n",
    "\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        ind_start_train = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[0]))[0][0]\n",
    "        ind_end_train = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[6]))[0][-1]\n",
    "        \n",
    "        start_ind_train.append(ind_start_train)\n",
    "        end_ind_train.append(ind_end_train)\n",
    "        \n",
    "    \n",
    "        ind_start_valid = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[8]))[0][0] \n",
    "        start_ind_valid.append(ind_start_valid)\n",
    "\n",
    "        ind_end_valid = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[8]))[0][-1]\n",
    "        end_ind_valid.append(ind_end_valid)\n",
    "\n",
    "\n",
    "        ind_start_test = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[9]))[0][0]\n",
    "        start_ind_test.append(ind_start_test)\n",
    "\n",
    "        ind_end_test = np.where((synced_times_datetimes_rounded_years == yr) & (synced_times_datetimes_rounded_months == new_month_order[9]))[0][-1]\n",
    "        end_ind_test.append(ind_end_test)       \n",
    "        \n",
    "    print('yr:', yr)\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "diff_train = np.array(end_ind_train) - np.array(start_ind_train)\n",
    "\n",
    "diff_valid = np.array(end_ind_valid) -  np.array(start_ind_valid)\n",
    "\n",
    "diff_test = np.array(end_ind_test) - np.array(start_ind_test)\n",
    "\n",
    "diff_train_sum = np.sum(diff_train)\n",
    "print('training data size:', diff_train_sum)\n",
    "\n",
    "diff_valid_sum = np.sum(diff_valid)\n",
    "print('validation data size:', diff_valid_sum)\n",
    "\n",
    "diff_test_sum = np.sum(diff_test)\n",
    "print('test data size:', diff_test_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data label statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total data avilable:', len(synced_times))\n",
    "print('expected amount of available data used in 7 months:', int(np.round((7./12)*len(synced_times_file_name),0))) #7 months of the year for training\n",
    "data_total = diff_train_sum + diff_valid_sum + diff_test_sum\n",
    "print('actual total data used:', data_total)\n",
    "print('perc. data used from total data available:', data_total/len(synced_times_file_name))\n",
    "print('perc. train:', diff_train_sum/data_total)\n",
    "print('perc. valid:', diff_valid_sum/data_total)\n",
    "print('perc. test:', diff_test_sum/data_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the HDF5 data cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Manually supply path to folder containing the synced HDF5 data cubes  \n",
    "\n",
    "Need to adjust input according to if haave 1, 3, or 7 SOHO products\n",
    "'''\n",
    "\n",
    "path_to_synced_HDF5 = '/home/carl/Documents/juniper_datacubes_3fams/All7SOHOProducts_19990101_20101231_6_6_128/'\n",
    "#'/home/carl/Documents/juniper_datacubes_3fams/MDI_19990101_20101231_6_6_128/'\n",
    "#'/home/carl/Documents/juniper_datacubes_3fams/All7SOHOProducts_19990101_20101231_6_6_128/'\n",
    "#'/home/carl/Documents/juniper_datacubes_3fams/MDI_EIT195_C2_19990101_20101231_6_6_128/'\n",
    "name_list = pattern_finder(path_to_synced_HDF5, pattern = '*sync.h5')\n",
    "print('cube name list:', name_list)\n",
    "print('len(name_list):', len(name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_list = []\n",
    "product_name_list = []\n",
    "for cu in name_list:\n",
    "    data_set = h5py.File(f'{path_to_synced_HDF5}{cu}','r')\n",
    "    data_set_keys = list(data_set.keys())[0]\n",
    "    if 'EIT' not in data_set_keys:\n",
    "        product_name_from_keys = ''.join(data_set_keys[:-1].split('_')[0:2]) \n",
    "    else:\n",
    "        product_name_from_keys = ''.join(data_set_keys[:-1].split('_')[0])        \n",
    "    print(data_set_keys)\n",
    "    data = data_set[str(list(data_set.keys())[0])][:]\n",
    "    print(np.shape(data))\n",
    "    print(np.where(data != data)[0])\n",
    "    \n",
    "    # Equalization\n",
    "    img_eq = exposure.equalize_adapthist(data[10], clip_limit=0.03)\n",
    "    \n",
    "    # Showing a representative image from each data cube: slice #10\n",
    "    plt.figure()\n",
    "    plt.imshow(img_eq, cmap='jet')\n",
    "    print('np.shape(data):', np.shape(data))\n",
    "    \n",
    "    #img_size = np.shape(data)[2]\n",
    "    #print('img_size:', img_size)\n",
    "    \n",
    "    cube_list.append(data)\n",
    "    product_name_list.append(product_name_from_keys)\n",
    "\n",
    "print('product_name_list', product_name_list)\n",
    "print('np.shape(cube_list):', np.shape(cube_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, combining the data from the rolling indices calculated in the preceding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Need to adjust input according to if haave 1, 3, or 7 SOHO products by uncommenting the corresponding\n",
    "lines in each of the two blocks in each of the three loops over the train, validate, and test sets\n",
    "\n",
    "For each SOHO product, the test set is normalized by the maximum product value found from training set \n",
    "\n",
    "Dimensions expanded to for batch dimension of tf tensor\n",
    "'''\n",
    "\n",
    "train_indices = list(zip(start_ind_train, end_ind_train))\n",
    "valid_indices = list(zip(start_ind_valid, end_ind_valid))\n",
    "test_indices = list(zip(start_ind_test, end_ind_test))\n",
    "\n",
    "#print('len(train_indices):', len(train_indices))\n",
    "#print('len(valid_indices):', len(valid_indices))\n",
    "#print('len(test_indices):', len(test_indices))\n",
    "\n",
    "SOHO_grand_train_set_1 = []\n",
    "SOHO_grand_train_set_2 = []\n",
    "SOHO_grand_train_set_3 = []\n",
    "SOHO_grand_train_set_4 = []\n",
    "SOHO_grand_train_set_5 = []\n",
    "SOHO_grand_train_set_6 = []\n",
    "SOHO_grand_train_set_7 = []\n",
    "\n",
    "SOHO_grand_valid_set_1 = []\n",
    "SOHO_grand_valid_set_2 = []\n",
    "SOHO_grand_valid_set_3 = []\n",
    "SOHO_grand_valid_set_4 = []\n",
    "SOHO_grand_valid_set_5 = []\n",
    "SOHO_grand_valid_set_6 = []\n",
    "SOHO_grand_valid_set_7 = []\n",
    "\n",
    "SOHO_grand_test_set_1 = []\n",
    "SOHO_grand_test_set_2 = []\n",
    "SOHO_grand_test_set_3 = []\n",
    "SOHO_grand_test_set_4 = []\n",
    "SOHO_grand_test_set_5 = []\n",
    "SOHO_grand_test_set_6 = []\n",
    "SOHO_grand_test_set_7 = []\n",
    "\n",
    "\n",
    "Bz_train_set = []\n",
    "Bz_valid_set = []\n",
    "Bz_test_set = []\n",
    "\n",
    "\n",
    "for i,j in train_indices:\n",
    "\n",
    "    \n",
    "    SOHO_grand_train_set_1 += list(cube_list[0][i:j])\n",
    "    SOHO_grand_train_set_2 += list(cube_list[1][i:j])\n",
    "    SOHO_grand_train_set_3 += list(cube_list[2][i:j])\n",
    "    SOHO_grand_train_set_4 += list(cube_list[3][i:j])\n",
    "    SOHO_grand_train_set_5 += list(cube_list[4][i:j])\n",
    "    SOHO_grand_train_set_6 += list(cube_list[5][i:j])\n",
    "    SOHO_grand_train_set_7 += list(cube_list[6][i:j])\n",
    "    Bz_train_set += list(omni2_Bzmin_minus_mean_3to5days_ahead[i:j])\n",
    "\n",
    "SOHO_grand_train_set_1_max = max(np.array(SOHO_grand_train_set_1).flat)\n",
    "SOHO_grand_train_set_2_max = max(np.array(SOHO_grand_train_set_2).flat)\n",
    "SOHO_grand_train_set_3_max = max(np.array(SOHO_grand_train_set_3).flat)\n",
    "SOHO_grand_train_set_4_max = max(np.array(SOHO_grand_train_set_4).flat)\n",
    "SOHO_grand_train_set_5_max = max(np.array(SOHO_grand_train_set_5).flat)\n",
    "SOHO_grand_train_set_6_max = max(np.array(SOHO_grand_train_set_6).flat)\n",
    "SOHO_grand_train_set_7_max = max(np.array(SOHO_grand_train_set_7).flat)\n",
    "                                 \n",
    "SOHO_grand_train_set_1_normed = np.expand_dims(SOHO_grand_train_set_1/SOHO_grand_train_set_1_max, axis=1)\n",
    "SOHO_grand_train_set_2_normed = np.expand_dims(SOHO_grand_train_set_2/SOHO_grand_train_set_2_max, axis=1)\n",
    "SOHO_grand_train_set_3_normed = np.expand_dims(SOHO_grand_train_set_3/SOHO_grand_train_set_3_max, axis=1)\n",
    "SOHO_grand_train_set_4_normed = np.expand_dims(SOHO_grand_train_set_4/SOHO_grand_train_set_4_max, axis=1)\n",
    "SOHO_grand_train_set_5_normed = np.expand_dims(SOHO_grand_train_set_5/SOHO_grand_train_set_5_max, axis=1)\n",
    "SOHO_grand_train_set_6_normed = np.expand_dims(SOHO_grand_train_set_6/SOHO_grand_train_set_6_max, axis=1)\n",
    "SOHO_grand_train_set_7_normed = np.expand_dims(SOHO_grand_train_set_7/SOHO_grand_train_set_7_max, axis=1)\n",
    "\n",
    "\n",
    "for i,j in valid_indices:\n",
    "    \n",
    "    SOHO_grand_valid_set_1 += list(cube_list[0][i:j])\n",
    "    SOHO_grand_valid_set_2 += list(cube_list[1][i:j])\n",
    "    SOHO_grand_valid_set_3 += list(cube_list[2][i:j])\n",
    "    SOHO_grand_valid_set_4 += list(cube_list[3][i:j])\n",
    "    SOHO_grand_valid_set_5 += list(cube_list[4][i:j])\n",
    "    SOHO_grand_valid_set_6 += list(cube_list[5][i:j])\n",
    "    SOHO_grand_valid_set_7 += list(cube_list[6][i:j])   \n",
    "    Bz_valid_set += list(omni2_Bzmin_minus_mean_3to5days_ahead[i:j])\n",
    "\n",
    "SOHO_grand_valid_set_1_normed = np.expand_dims(SOHO_grand_valid_set_1/SOHO_grand_train_set_1_max, axis=1)\n",
    "SOHO_grand_valid_set_2_normed = np.expand_dims(SOHO_grand_valid_set_2/SOHO_grand_train_set_2_max, axis=1)\n",
    "SOHO_grand_valid_set_3_normed = np.expand_dims(SOHO_grand_valid_set_3/SOHO_grand_train_set_3_max, axis=1)\n",
    "SOHO_grand_valid_set_4_normed = np.expand_dims(SOHO_grand_valid_set_4/SOHO_grand_train_set_4_max, axis=1)\n",
    "SOHO_grand_valid_set_5_normed = np.expand_dims(SOHO_grand_valid_set_5/SOHO_grand_train_set_5_max, axis=1)\n",
    "SOHO_grand_valid_set_6_normed = np.expand_dims(SOHO_grand_valid_set_6/SOHO_grand_train_set_6_max, axis=1)\n",
    "SOHO_grand_valid_set_7_normed = np.expand_dims(SOHO_grand_valid_set_7/SOHO_grand_train_set_7_max, axis=1)\n",
    "\n",
    "for i,j in test_indices:\n",
    "    \n",
    "    SOHO_grand_test_set_1 += list(cube_list[0][i:j])\n",
    "    SOHO_grand_test_set_2 += list(cube_list[1][i:j])\n",
    "    SOHO_grand_test_set_3 += list(cube_list[2][i:j])\n",
    "    SOHO_grand_test_set_4 += list(cube_list[3][i:j])\n",
    "    SOHO_grand_test_set_5 += list(cube_list[4][i:j])\n",
    "    SOHO_grand_test_set_6 += list(cube_list[5][i:j])\n",
    "    SOHO_grand_test_set_7 += list(cube_list[6][i:j])     \n",
    "    Bz_test_set += list(omni2_Bzmin_minus_mean_3to5days_ahead[i:j])\n",
    "    \n",
    "SOHO_grand_test_set_1_normed = np.expand_dims(SOHO_grand_test_set_1/SOHO_grand_train_set_1_max, axis=1)\n",
    "SOHO_grand_test_set_2_normed = np.expand_dims(SOHO_grand_test_set_2/SOHO_grand_train_set_2_max, axis=1)\n",
    "SOHO_grand_test_set_3_normed = np.expand_dims(SOHO_grand_test_set_3/SOHO_grand_train_set_3_max, axis=1)\n",
    "SOHO_grand_test_set_4_normed = np.expand_dims(SOHO_grand_test_set_4/SOHO_grand_train_set_4_max, axis=1)\n",
    "SOHO_grand_test_set_5_normed = np.expand_dims(SOHO_grand_test_set_5/SOHO_grand_train_set_5_max, axis=1)\n",
    "SOHO_grand_test_set_6_normed = np.expand_dims(SOHO_grand_test_set_6/SOHO_grand_train_set_6_max, axis=1)\n",
    "SOHO_grand_test_set_7_normed = np.expand_dims(SOHO_grand_test_set_7/SOHO_grand_train_set_7_max, axis=1)\n",
    "\n",
    "#double checking that same length train, validation, and test sets as previously found.   \n",
    "print('np.shape(SOHO_grand_train_set_1):', np.shape(SOHO_grand_train_set_1))\n",
    "#print(len(SOHO_grand_train_set_1) + len(SOHO_grand_train_set_2) + len(SOHO_grand_train_set_3)) #+ len(SOHO_grand_train_set_4) + len(SOHO_grand_train_set_5) + len(SOHO_grand_train_set_6) + len(SOHO_grand_train_set_7))\n",
    "\n",
    "print('np.shape(SOHO_grand_valid_set_1):', np.shape(SOHO_grand_valid_set_1))\n",
    "#print(len(SOHO_grand_valid_set_1) + len(SOHO_grand_valid_set_2) + len(SOHO_grand_valid_set_3)) #+ len(SOHO_grand_valid_set_4) + len(SOHO_grand_valid_set_5) + len(SOHO_grand_valid_set_6) + len(SOHO_grand_valid_set_7))\n",
    "\n",
    "print('np.shape(SOHO_grand_test_set_1):', np.shape(SOHO_grand_test_set_1))\n",
    "#print(len(SOHO_grand_test_set_1) + len(SOHO_grand_test_set_2) + len(SOHO_grand_test_set_3)) #+ len(SOHO_grand_test_set_4) + len(SOHO_grand_test_set_5) + len(SOHO_grand_test_set_6) + len(SOHO_grand_test_set_7))\n",
    "\n",
    "\n",
    "Bz_train_set_len = len(Bz_train_set)\n",
    "Bz_valid_set_len = len(Bz_valid_set)\n",
    "Bz_test_set_len = len(Bz_test_set)\n",
    "\n",
    "print('len(Bz_train_set):', Bz_train_set_len)\n",
    "print('len(Bz_valid_set):', Bz_valid_set_len)\n",
    "print('len(Bz_test_set):', Bz_test_set_len)\n",
    "print('total data size:', Bz_train_set_len + Bz_valid_set_len + Bz_test_set_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating threshold Bz values for MDI only family training set. For consistency will apply these Bz values to binarize the validation and test set within MDI only as well as the other two experiments (MDI + Lasco C2 + EIT195, and all 7 SOHO products together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ONLY USE IF ONLY HAVE MDI PRODUCT.\n",
    "\n",
    "PERFORM ONLY FOR MDI_ONLY TRAINING SET\n",
    "'''\n",
    "\n",
    "thresholds = list(np.arange(5,30,5)/100.)\n",
    "print('thresholds:', thresholds)\n",
    "\n",
    "Bz_train_set_sorted = np.sort(Bz_train_set) #most negative to least negative values\n",
    "\n",
    "p_val_thres = np.round(np.arange(len(Bz_train_set_sorted)) / (len(Bz_train_set_sorted) - 1.),4)\n",
    "\n",
    "ind_p = [np.where(p_val_thres == (elem + np.min(np.abs(p_val_thres - elem))))[0][0] for elem in thresholds]\n",
    "\n",
    "Bz_train_set_sorted_thresholds = [Bz_train_set_sorted[elem] for elem in ind_p]\n",
    "print('Bz_train_set_sorted_thresholds:', Bz_train_set_sorted_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bz values are -11.6, -8.91, -7.76, -6.94, and -6.27 nT @ (5,10,15,20,25)% threshold obtained from the MDI only experiment.\n",
    "\n",
    "These thresholds are fixed for all three experiments. \n",
    "\n",
    "Binarize the ground truth labels accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Binarize training, validation and test sets\n",
    "\n",
    "Compute Class Weights\n",
    "'''\n",
    "\n",
    "Bz_train_per_threshold_list = []\n",
    "Bz_valid_per_threshold_list = []\n",
    "Bz_test_per_threshold_list = []\n",
    "\n",
    "class_weights_list = []\n",
    "\n",
    "Bz_thresholds_on_train_MDIonly = [-6.27, -6.94, -7.76, -8.91, -11.6]\n",
    "\n",
    "for val in Bz_thresholds_on_train_MDIonly:\n",
    "    \n",
    "    Bz_train_set_copy = np.copy(Bz_train_set)\n",
    "    Bz_valid_set_copy = np.copy(Bz_valid_set)\n",
    "    Bz_test_set_copy = np.copy(Bz_test_set)\n",
    "    \n",
    "    ind1_train = np.where(np.array(Bz_train_set_copy) <= val)[0]\n",
    "    #print('len(ind1_train):', len(ind1_train))\n",
    "    ind0_train = np.where(np.array(Bz_train_set_copy) > val)[0]\n",
    "    #print('len(ind0_train):', len(ind0_train))\n",
    "    \n",
    "    ind1_valid = np.where(np.array(Bz_valid_set_copy) <= val)[0]\n",
    "    #print('len(ind1_valid):', len(ind1_valid))\n",
    "    ind0_valid = np.where(np.array(Bz_valid_set_copy) > val)[0]\n",
    "    #print('len(ind0_valid):', len(ind0_valid))\n",
    "    \n",
    "    ind1_test = np.where(np.array(Bz_test_set_copy) <= val)[0]\n",
    "    #print('len(ind1_test):', len(ind1_test))\n",
    "    ind0_test = np.where(np.array(Bz_test_set_copy) > val)[0]\n",
    "    #print('len(ind0_test):', len(ind0_test))\n",
    "    \n",
    "    \n",
    "    Bz_train_set_copy[ind1_train] = 1 #the events\n",
    "    Bz_train_set_copy[ind0_train] = 0 #the non-events\n",
    "    \n",
    "    Bz_valid_set_copy[ind1_valid] = 1 #the events\n",
    "    Bz_valid_set_copy[ind0_valid] = 0 #the non-events\n",
    "    \n",
    "    Bz_test_set_copy[ind1_test] = 1 #the events\n",
    "    Bz_test_set_copy[ind0_test] = 0 #the non-events    \n",
    "    \n",
    "    Bz_train_per_threshold_list += list(Bz_train_set_copy)\n",
    "    Bz_valid_per_threshold_list += list(Bz_valid_set_copy)\n",
    "    Bz_test_per_threshold_list += list(Bz_test_set_copy)\n",
    "    \n",
    "    class_weights_train = class_weight.compute_class_weight('balanced', np.unique(Bz_train_set_copy), Bz_train_set_copy)\n",
    "    class_weights_list.append(class_weights_train)\n",
    "\n",
    "print('class_weights_list:', class_weights_list)\n",
    "\n",
    "print('np.shape(Bz_train_per_threshold_list):', np.shape(Bz_train_per_threshold_list)) \n",
    "print('np.shape(Bz_valid_per_threshold_list):', np.shape(Bz_valid_per_threshold_list)) \n",
    "print('np.shape(Bz_test_per_threshold_list):', np.shape(Bz_test_per_threshold_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data ready and loaded, we are ready to conduct the three ML experiments with a deep CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TF version:', tf.__version__) ### check the TF API for your version of TF\n",
    "\n",
    "# NEED THIS TO BE ABLE TO PEER INSIDE WITH .NUMPY() and to return results immediately\n",
    "#tf.enable_eager_execution()\n",
    "#tf.executing_eagerly()\n",
    "\n",
    "\n",
    "### SET THE RANDOM SEED\n",
    "tf.random.set_random_seed(1234) #this for TF version 1.12.0\n",
    "np.random.seed(0)\n",
    "\n",
    "### THIS IS A TEST TO MAKE SURE THAT \"tf.random.set_random_seed(1234)\" INPUT ABOVE SETS THE INITIALIZER \n",
    "initializer = tf.keras.initializers.glorot_uniform(seed=None)\n",
    "values = initializer(shape=(3, 3))\n",
    "print(tf.Session().run(values)) #.eval()\n",
    "print('np.random.rand(3,2):', np.random.rand(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Currently the learning rate is a constant\n",
    "'''\n",
    "\n",
    "### epochs set to 1 as a test. Change this! \n",
    "\n",
    "epochs = 1 #100\n",
    "lr = 0.01 #.01 for SGD optimizer, .001 for Adam optimizer\n",
    "mini_batch = 64\n",
    "img_size = np.shape(data)[2]\n",
    "pool_size_factor = int(img_size/32) #to get the input image dimension down to size 32 x 32 for this example network\n",
    "print('epochs:', epochs, 'learning rate:', lr, 'mini_batch size:', mini_batch, 'image y-dim:', img_size, 'pool_size_factor:', pool_size_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optional: freeing up memory held by these variables\n",
    "'''\n",
    "\n",
    "cube_list = None \n",
    "omni2_Bzmin_minus_mean_3to5days_ahead = None \n",
    "data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base unit block architecture for all 7 products: will have seven seperate models. Each architecture is fixed per product! Trainable = True, so layers not frozen! Seed=None in kernel_initializer because it has already been fixed by tf.random.set_random_seed(1234) above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setting the three-layer architecture to be input into tf.keras.Sequential()\n",
    "'''\n",
    "\n",
    "architecture_list = []\n",
    "\n",
    "for prodname in product_name_list: \n",
    "    \n",
    "    architecture = [\n",
    "        tf.keras.layers.InputLayer(\n",
    "                input_shape = (1, img_size, img_size),\n",
    "                name = f'conv2d0_{prodname}_input'), \n",
    "\n",
    "        tf.keras.layers.MaxPooling2D(pool_size = (pool_size_factor, pool_size_factor), strides = pool_size_factor, padding = 'same', data_format = 'channels_first', name = f'maxpool_{prodname}'), \n",
    "        tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), use_bias=False, data_format = 'channels_first', padding = 'same', trainable = True, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=None), name = f'conv2d1_{prodname}'),\n",
    "        tf.keras.layers.BatchNormalization(axis=1, trainable = True, name = f'BN1_{prodname}'),\n",
    "        tf.keras.layers.ReLU(name = f'ReLU1_{prodname}'), \n",
    "        tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2, padding = 'same', data_format = 'channels_first', name = f'maxpl2_{prodname}'), \n",
    "\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), use_bias=False, padding = 'same', data_format = 'channels_first', trainable = True, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=None), name = f'conv2d2_{prodname}'),\n",
    "        tf.keras.layers.BatchNormalization(axis=1, trainable = True, name = f'BN2_{prodname}'),\n",
    "        tf.keras.layers.ReLU(name = f'ReLU2_{prodname}'), \n",
    "        tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2, padding = 'same', data_format = 'channels_first', name = f'maxpl3_{prodname}'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(16, (3, 3), use_bias=False, padding = 'same', data_format = 'channels_first', trainable = True, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=None), name = f'conv2d3_{prodname}'),\n",
    "        tf.keras.layers.BatchNormalization(axis=1, trainable = True, name = f'BN3_{prodname}'),\n",
    "        tf.keras.layers.ReLU(name = f'ReLU3_{prodname}'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2, padding = 'same', data_format = 'channels_first', name = f'maxpl4_{prodname}'),\n",
    "\n",
    "        tf.keras.layers.Flatten(name = f'flatten_{prodname}')\n",
    "        ]\n",
    "    \n",
    "    architecture_list.append(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Keras model\n",
    "\n",
    "Need to adjust input according to if haave 1, 3, or 7 SOHO products by uncommenting the corresponding\n",
    "'''\n",
    "\n",
    "model_pre_list = [] #this is in the case where want to load weights later\n",
    "model_pre_input_list = []\n",
    "model_pre_output_list = []\n",
    "for elem in range(len(product_name_list)):\n",
    "    print('elem:', elem)\n",
    "    model_pre = tf.keras.Sequential(architecture_list[elem]) \n",
    "    model_pre_list.append(model_pre)\n",
    "    model_pre_input_list.append(model_pre.input)\n",
    "    model_pre_output_list.append(model_pre.output)\n",
    "\n",
    "#For 1 product:\n",
    "#merged_pre_model = model_pre.output\n",
    "\n",
    "#For 3 and 7 products:\n",
    "merged_pre_model = tf.keras.layers.concatenate(model_pre_output_list, name ='concat')\n",
    "\n",
    "\n",
    "pred1densepre_BN = tf.keras.layers.BatchNormalization(renorm=False, trainable = True, name = 'BN_concat')(merged_pre_model)\n",
    "pred1dense = tf.keras.layers.Dense(units = 128, use_bias=False, trainable = True, kernel_regularizer = tf.keras.regularizers.l2(0.), kernel_initializer = tf.keras.initializers.glorot_uniform(seed=0), name = 'dense1')(pred1densepre_BN)\n",
    "pre1dense_ReLU1 = tf.keras.layers.ReLU(name = 'ReLU4')(pred1dense) #256 #128\n",
    "pre1dense_ReLU1_dropout1 = tf.keras.layers.Dropout(rate=0.6, name = 'drop1')(pre1dense_ReLU1)\n",
    "pred2dense = tf.keras.layers.Dense(units = 32, activation = 'linear', use_bias=False, trainable = True, kernel_regularizer = tf.keras.regularizers.l2(0.), kernel_initializer =   tf.keras.initializers.glorot_uniform(seed=None), name = 'dense2')(pre1dense_ReLU1_dropout1)\n",
    "pred2dense_ReLU2 = tf.keras.layers.ReLU(name = 'ReLU5')(pred2dense) #64 #32\n",
    "pred2dense_ReLU2_dropout2 = tf.keras.layers.Dropout(rate=0.6, name = 'drop2')(pred2dense_ReLU2)\n",
    "pred3dense = tf.keras.layers.Dense(units = 1, activation = 'sigmoid', use_bias=True, trainable = True, kernel_regularizer = tf.keras.regularizers.l2(0.), kernel_initializer = tf.keras.initializers.glorot_uniform(seed=None), name = 'dense3')(pred2dense_ReLU2_dropout2)\n",
    "\n",
    "# THE ACTUAL AUGMENETED MODEL\n",
    "model = tf.keras.models.Model(model_pre_input_list, pred3dense)\n",
    "\n",
    "# Clearing memory\n",
    "model_pre_input_list = None\n",
    "model_pre_output_list = None\n",
    "\n",
    "# Create model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with some loss function and optimizer #0.01 is default lr for SGD \n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.binary_crossentropy, \n",
    "    optimizer = tf.keras.optimizers.SGD(lr = lr, decay = 1e-3, momentum=0.8, nesterov=False),\n",
    "    metrics = ['accuracy'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    "Enable early stopping and saving of model checkpoints\n",
    "'''\n",
    "\n",
    "experiment_dir = os.getcwd() #or user defined\n",
    "\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = experiment_dir,\n",
    "    write_graph = True,\n",
    "    write_images = True)\n",
    "\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    experiment_dir+\"/model-weights-{epoch:02d}.hdf5\", \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 0, \n",
    "    save_best_only = True, \n",
    "    save_weights_only = False, \n",
    "    mode = 'auto', period = 10)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0,\n",
    "    patience = 10,\n",
    "    verbose = 0,\n",
    "    mode = 'auto',\n",
    "    baseline = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train, Validation, and Test labels per each of the five thresholds\n",
    "'''\n",
    "\n",
    "per_threshold_given_products_train_labels = np.array_split(Bz_train_per_threshold_list, len(Bz_thresholds_on_train_MDIonly)) \n",
    "per_threshold_given_products_valid_labels = np.array_split(Bz_valid_per_threshold_list, len(Bz_thresholds_on_train_MDIonly)) \n",
    "per_threshold_given_products_test_labels = np.array_split(Bz_test_per_threshold_list, len(Bz_thresholds_on_train_MDIonly)) \n",
    "\n",
    "print('train labels for all threholds:', np.shape(per_threshold_given_products_train_labels))\n",
    "print('validation labels for all threholds:',np.shape(per_threshold_given_products_valid_labels))\n",
    "print('test labels for all threholds:', np.shape(per_threshold_given_products_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Running CNN network for the specified number of epochs\n",
    "followed by Model fitting on the training and validation sets and Model predictions on the test set\n",
    "Obtain TSS and MCC scores\n",
    "\n",
    "NEED TO ADJUST FOR 1,3, OR 7 PRODUCTS\n",
    "'''\n",
    "\n",
    "for exper, thres in tqdm(enumerate(Bz_thresholds_on_train_MDIonly)): #outerloop to perform an ML experiment per threshold\n",
    "    print(f'results for threshold {thres}')\n",
    "    print('class_weights_list[exper]:', class_weights_list[exper])\n",
    "    model.fit(\n",
    "        \n",
    "        #For 1 product:\n",
    "        #x = SOHO_grand_train_set_1_normed, \n",
    "        \n",
    "        #for all 3 products:\n",
    "        #x = list((SOHO_grand_train_set_1_normed, SOHO_grand_train_set_2_normed, SOHO_grand_train_set_3_normed)),\n",
    "        \n",
    "        #for all 7 products:\n",
    "        x = list((SOHO_grand_train_set_1_normed, SOHO_grand_train_set_2_normed, SOHO_grand_train_set_3_normed, SOHO_grand_train_set_4_normed, SOHO_grand_train_set_5_normed, SOHO_grand_train_set_6_normed, SOHO_grand_train_set_7_normed)),\n",
    "        \n",
    "        y = per_threshold_given_products_train_labels[exper], \n",
    "        epochs = epochs,\n",
    "        batch_size = mini_batch,\n",
    "        shuffle = True,\n",
    "        initial_epoch=0,\n",
    "        steps_per_epoch = None,\n",
    "        verbose = 1,\n",
    "        validation_data = (\n",
    "        \n",
    "        #For 1 product:\n",
    "        #SOHO_grand_valid_set_1_normed,per_threshold_given_products_valid_labels[exper]),\n",
    "        \n",
    "        #For all 3 products:\n",
    "        #list((SOHO_grand_valid_set_1_normed, SOHO_grand_valid_set_2_normed, SOHO_grand_valid_set_3_normed)), per_threshold_given_products_valid_labels[exper]),\n",
    "        \n",
    "        #For all 7 products:\n",
    "        list((SOHO_grand_valid_set_1_normed, SOHO_grand_valid_set_2_normed, SOHO_grand_valid_set_3_normed, SOHO_grand_valid_set_4_normed, SOHO_grand_valid_set_5_normed, SOHO_grand_valid_set_6_normed, SOHO_grand_valid_set_7_normed)), per_threshold_given_products_valid_labels[exper]),\n",
    "        \n",
    "        class_weight = class_weights_list[exper],\n",
    "        validation_steps = None,\n",
    "        callbacks = [tb, cp, es] #so with early stopping!\n",
    "    )\n",
    "    #Having the model make the predictions\n",
    "    predictions_test_data = model.predict(\n",
    "        \n",
    "        #For 1 product:\n",
    "        #SOHO_grand_test_set_1_normed,\n",
    "        \n",
    "        #For all 3 products:\n",
    "        #list((SOHO_grand_test_set_1_normed, SOHO_grand_test_set_2_normed, SOHO_grand_test_set_3_normed)),\n",
    "        \n",
    "        #For all 7 products:\n",
    "        list((SOHO_grand_test_set_1_normed, SOHO_grand_test_set_2_normed, SOHO_grand_test_set_3_normed, SOHO_grand_test_set_4_normed, SOHO_grand_test_set_5_normed, SOHO_grand_test_set_6_normed, SOHO_grand_test_set_7_normed)),\n",
    "        \n",
    "        batch_size=mini_batch,\n",
    "        steps = None,\n",
    "        verbose = 1)\n",
    "\n",
    "    predictions_train_data = model.predict(\n",
    "        \n",
    "        #For 1 product:\n",
    "        #SOHO_grand_train_set_1_normed,\n",
    "        \n",
    "        #For all 3 products:\n",
    "        #list((SOHO_grand_train_set_1_normed, SOHO_grand_train_set_2_normed, SOHO_grand_train_set_3_normed)),\n",
    "        \n",
    "        #For all 7 products:\n",
    "        list((SOHO_grand_train_set_1_normed, SOHO_grand_train_set_2_normed, SOHO_grand_train_set_3_normed, SOHO_grand_train_set_4_normed, SOHO_grand_train_set_5_normed, SOHO_grand_train_set_6_normed, SOHO_grand_train_set_7_normed)),\n",
    "        \n",
    "        batch_size=mini_batch,\n",
    "        steps = None,\n",
    "        verbose = 1)\n",
    "\n",
    "    predictions_valid_data = model.predict(\n",
    "        \n",
    "        #For 1 product:\n",
    "        #SOHO_grand_valid_set_1_normed,\n",
    "        \n",
    "        #For all 3 products:\n",
    "        #list((SOHO_grand_valid_set_1_normed, SOHO_grand_valid_set_2_normed, SOHO_grand_valid_set_3_normed)),        \n",
    "        \n",
    "        #For all 7 products:\n",
    "        list((SOHO_grand_valid_set_1_normed, SOHO_grand_valid_set_2_normed, SOHO_grand_valid_set_3_normed, SOHO_grand_valid_set_4_normed, SOHO_grand_valid_set_5_normed, SOHO_grand_valid_set_6_normed, SOHO_grand_valid_set_7_normed)),\n",
    "        \n",
    "        batch_size=mini_batch,\n",
    "        steps = None,\n",
    "        verbose = 1)\n",
    "    \n",
    "    #### CHOOSING THE 50% CUTOFF PROBABILITY\n",
    "    predictions_test_data_copy = np.copy(predictions_test_data)\n",
    "    ind_0_pred_test = np.where(np.array(predictions_test_data_copy) < 0.5)[0]\n",
    "    ind_1_pred_test = np.where(np.array(predictions_test_data_copy) >= 0.5)[0]\n",
    "    predictions_test_data_copy[ind_0_pred_test] = 0\n",
    "    predictions_test_data_copy[ind_1_pred_test] = 1\n",
    "    #print('predictions_test_data_copy[0:10]:', predictions_test_data_copy[0:10])\n",
    "    \n",
    "    predictions_train_data_copy = np.copy(predictions_train_data)\n",
    "    ind_0_pred_train = np.where(np.array(predictions_train_data_copy) < 0.5)[0]\n",
    "    ind_1_pred_train = np.where(np.array(predictions_train_data_copy) >= 0.5)[0]\n",
    "    predictions_train_data_copy[ind_0_pred_train] = 0\n",
    "    predictions_train_data_copy[ind_1_pred_train] = 1\n",
    "    #print('predictions_train_data_copy[0:10]:', predictions_train_data_copy[0:10])\n",
    "    \n",
    "    predictions_valid_data_copy = np.copy(predictions_valid_data)\n",
    "    ind_0_pred_valid = np.where(np.array(predictions_valid_data_copy) < 0.5)[0]\n",
    "    ind_1_pred_valid = np.where(np.array(predictions_valid_data_copy) >= 0.5)[0]\n",
    "    predictions_valid_data_copy[ind_0_pred_valid] = 0\n",
    "    predictions_valid_data_copy[ind_1_pred_valid] = 1\n",
    "    #print('predictions_train_data_copy[0:10]:', predictions_valid_data_copy[0:10])\n",
    "    \n",
    "    confu_matrix_on_train = confusion_matrix(per_threshold_given_products_train_labels[exper], predictions_train_data_copy)\n",
    "    print('confu_matrix_on_train:\\n', confu_matrix_on_train)\n",
    "    confu_matrix_on_valid = confusion_matrix(per_threshold_given_products_valid_labels[exper], predictions_valid_data_copy)\n",
    "    print('confu_matrix_on_valid:\\n', confu_matrix_on_valid)\n",
    "    confu_matrix_on_test = confusion_matrix(per_threshold_given_products_test_labels[exper], predictions_test_data_copy)\n",
    "    print('confu_matrix_on_test:\\n', confu_matrix_on_test)\n",
    "    \n",
    "    matthews_on_test = matthews_corrcoef(per_threshold_given_products_test_labels[exper], predictions_test_data_copy)\n",
    "    print('matthews_on_test:', matthews_on_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(per_threshold_given_products_test_labels[exper], predictions_test_data_copy).ravel()\n",
    "    print('tn, fp, fn, tp:', tn, fp, fn, tp)\n",
    "    TSS_on_test = tp/(tp + fn) - fp/(fp + tn)\n",
    "    #matthews_on_test = ( (tp*tn)-(fp*fn) ) / np.sqrt((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn)) #manual version\n",
    "    #print('matthews_on_test:', matthews_on_test)\n",
    "    print('TSS_on_test:', TSS_on_test)\n",
    "    print('***next threshold***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "model.save_weights(f\"{experiment_dir}/model-{epochs}_Prods-{len(name_list)}_batches-{mini_batch}_size-{img_size}_{datetime.now().strftime('%Y-%m-%d-%H-%M')}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for Gaussian Naive Bayes analysis\n",
    "Approach is deterministic and so procedures applied to find feature combination on the train set directly hold\n",
    "on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NEED TO ADJUST FOR 1,3, OR 7 PRODUCTS\n",
    "'''\n",
    "\n",
    "SOHO_grand_train_set_1_normed_squeeze = np.squeeze(SOHO_grand_train_set_1_normed)\n",
    "SOHO_grand_test_set_1_normed_squeeze = np.squeeze(SOHO_grand_test_set_1_normed)\n",
    "\n",
    "SOHO_grand_train_set_2_normed_squeeze = np.squeeze(SOHO_grand_train_set_2_normed)\n",
    "SOHO_grand_test_set_2_normed_squeeze = np.squeeze(SOHO_grand_test_set_2_normed)\n",
    "\n",
    "SOHO_grand_train_set_3_normed_squeeze = np.squeeze(SOHO_grand_train_set_3_normed)\n",
    "SOHO_grand_test_set_3_normed_squeeze = np.squeeze(SOHO_grand_test_set_3_normed)\n",
    "\n",
    "SOHO_grand_train_set_4_normed_squeeze = np.squeeze(SOHO_grand_train_set_4_normed)\n",
    "SOHO_grand_test_set_4_normed_squeeze = np.squeeze(SOHO_grand_test_set_4_normed)\n",
    "\n",
    "SOHO_grand_train_set_5_normed_squeeze = np.squeeze(SOHO_grand_train_set_5_normed)\n",
    "SOHO_grand_test_set_5_normed_squeeze = np.squeeze(SOHO_grand_test_set_5_normed)\n",
    "\n",
    "SOHO_grand_train_set_6_normed_squeeze = np.squeeze(SOHO_grand_train_set_6_normed)\n",
    "SOHO_grand_test_set_6_normed_squeeze = np.squeeze(SOHO_grand_test_set_6_normed)\n",
    "\n",
    "SOHO_grand_train_set_7_normed_squeeze = np.squeeze(SOHO_grand_train_set_7_normed)\n",
    "SOHO_grand_test_set_7_normed_squeeze = np.squeeze(SOHO_grand_test_set_7_normed)\n",
    "\n",
    "print('np.shape(SOHO_grand_train_set_1_normed_squeez):', np.shape(SOHO_grand_train_set_1_normed_squeeze))\n",
    "print('np.shape(SOHO_grand_test_set_1_normed_squeeze):', np.shape(SOHO_grand_test_set_1_normed_squeeze))\n",
    "\n",
    "#print('np.shape(SOHO_grand_train_set_2_normed_squeez):', np.shape(SOHO_grand_train_set_2_normed_squeeze))\n",
    "#print('np.shape(SOHO_grand_test_set_2_normed_squeeze):', np.shape(SOHO_grand_test_set_2_normed_squeeze))\n",
    "\n",
    "#print('np.shape(SOHO_grand_train_set_3_normed_squeez):', np.shape(SOHO_grand_train_set_3_normed_squeeze))\n",
    "#print('np.shape(SOHO_grand_test_set_3_normed_squeeze):', np.shape(SOHO_grand_test_set_3_normed_squeeze))\n",
    "\n",
    "#print('np.shape(SOHO_grand_train_set_4_normed_squeez):', np.shape(SOHO_grand_train_set_4_normed_squeeze))\n",
    "#print('np.shape(SOHO_grand_test_set_4_normed_squeeze):', np.shape(SOHO_grand_test_set_4_normed_squeeze))\n",
    "\n",
    "#print('np.shape(SOHO_grand_train_set_5_normed_squeez):', np.shape(SOHO_grand_train_set_5_normed_squeeze))\n",
    "#print('np.shape(SOHO_grand_test_set_5_normed_squeeze):', np.shape(SOHO_grand_test_set_5_normed_squeeze))\n",
    "\n",
    "#print('np.shape(SOHO_grand_train_set_6_normed_squeez):', np.shape(SOHO_grand_train_set_6_normed_squeeze))\n",
    "#print('np.shape(SOHO_grand_test_set_6_normed_squeeze):', np.shape(SOHO_grand_test_set_6_normed_squeeze))\n",
    "\n",
    "#print('np.shape(SOHO_grand_train_set_7_normed_squeez):', np.shape(SOHO_grand_train_set_7_normed_squeeze))\n",
    "#print('np.shape(SOHO_grand_test_set_7_normed_squeeze):', np.shape(SOHO_grand_test_set_7_normed_squeeze))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NEED TO ADJUST FOR 1,3, OR 7 PRODUCTS\n",
    "'''\n",
    "\n",
    "data_array_1_tr = []\n",
    "data_array_2_tr = []\n",
    "data_array_3_tr = []\n",
    "data_array_4_tr = []\n",
    "data_array_5_tr = []\n",
    "data_array_6_tr = []\n",
    "data_array_7_tr = []\n",
    "\n",
    "data_array_1_te = []\n",
    "data_array_2_te = []\n",
    "data_array_3_te = []\n",
    "data_array_4_te = []\n",
    "data_array_5_te = []\n",
    "data_array_6_te = []\n",
    "data_array_7_te = []\n",
    "\n",
    "\n",
    "for l, elem in enumerate(SOHO_grand_train_set_1_normed_squeeze):\n",
    "    data_array_1_tr.append(stats_mbfracdim(SOHO_grand_train_set_1_normed_squeeze[l],np.nanmean(SOHO_grand_train_set_1_normed_squeeze[l]),-1))\n",
    "    data_array_2_tr.append(stats_mbfracdim(SOHO_grand_train_set_2_normed_squeeze[l],np.nanmean(SOHO_grand_train_set_2_normed_squeeze[l]),-1))\n",
    "    data_array_3_tr.append(stats_mbfracdim(SOHO_grand_train_set_3_normed_squeeze[l],np.nanmean(SOHO_grand_train_set_3_normed_squeeze[l]),-1))\n",
    "    data_array_4_tr.append(stats_mbfracdim(SOHO_grand_train_set_4_normed_squeeze[l],np.nanmean(SOHO_grand_train_set_4_normed_squeeze[l]),-1))\n",
    "    data_array_5_tr.append(stats_mbfracdim(SOHO_grand_train_set_5_normed_squeeze[l],np.nanmean(SOHO_grand_train_set_5_normed_squeeze[l]),-1))\n",
    "    data_array_6_tr.append(stats_mbfracdim(SOHO_grand_train_set_6_normed_squeeze[l],np.nanmean(SOHO_grand_train_set_6_normed_squeeze[l]),-1))\n",
    "    data_array_7_tr.append(stats_mbfracdim(SOHO_grand_train_set_7_normed_squeeze[l],np.nanmean(SOHO_grand_train_set_7_normed_squeeze[l]),-1))\n",
    "\n",
    "for l, elem in enumerate(SOHO_grand_test_set_1_normed_squeeze):\n",
    "    data_array_1_te.append(stats_mbfracdim(SOHO_grand_test_set_1_normed_squeeze[l],np.nanmean(SOHO_grand_test_set_1_normed_squeeze[l]),-1))\n",
    "    data_array_2_te.append(stats_mbfracdim(SOHO_grand_test_set_2_normed_squeeze[l],np.nanmean(SOHO_grand_test_set_2_normed_squeeze[l]),-1))\n",
    "    data_array_3_te.append(stats_mbfracdim(SOHO_grand_test_set_3_normed_squeeze[l],np.nanmean(SOHO_grand_test_set_3_normed_squeeze[l]),-1))\n",
    "    data_array_4_te.append(stats_mbfracdim(SOHO_grand_test_set_4_normed_squeeze[l],np.nanmean(SOHO_grand_test_set_4_normed_squeeze[l]),-1))\n",
    "    data_array_5_te.append(stats_mbfracdim(SOHO_grand_test_set_5_normed_squeeze[l],np.nanmean(SOHO_grand_test_set_5_normed_squeeze[l]),-1))\n",
    "    data_array_6_te.append(stats_mbfracdim(SOHO_grand_test_set_6_normed_squeeze[l],np.nanmean(SOHO_grand_test_set_6_normed_squeeze[l]),-1))\n",
    "    data_array_7_te.append(stats_mbfracdim(SOHO_grand_test_set_7_normed_squeeze[l],np.nanmean(SOHO_grand_test_set_7_normed_squeeze[l]),-1))\n",
    "    \n",
    "\n",
    "print('np.shape(data_array_1_tr):', np.shape(data_array_1_tr))\n",
    "#print('np.shape(data_array_2_tr):', np.shape(data_array_2_tr))\n",
    "#print('np.shape(data_array_3_tr):', np.shape(data_array_3_tr))\n",
    "#print('np.shape(data_array_4_tr):', np.shape(data_array_4_tr))\n",
    "#print('np.shape(data_array_5_tr):', np.shape(data_array_5_tr))\n",
    "#print('np.shape(data_array_6_tr):', np.shape(data_array_6_tr))\n",
    "#print('np.shape(data_array_7_tr):', np.shape(data_array_7_tr))\n",
    "\n",
    "print('np.shape(data_array_1_te):', np.shape(data_array_1_te))\n",
    "#print('np.shape(data_array_2_te):', np.shape(data_array_2_te))\n",
    "#print('np.shape(data_array_3_te):', np.shape(data_array_3_te))\n",
    "#print('np.shape(data_array_4_te):', np.shape(data_array_4_te))\n",
    "#print('np.shape(data_array_5_te):', np.shape(data_array_5_te))\n",
    "#print('np.shape(data_array_6_te):', np.shape(data_array_6_te))\n",
    "#print('np.shape(data_array_7_te):', np.shape(data_array_7_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*** SKIP IF 1 PRODUCT\n",
    "\n",
    "NEED TO ADJUST FOR 3 or 7 PRODUCTS!\n",
    "\n",
    "Generates all combinations of the max, min, mean, std, and frac features for the 3 (216) \n",
    "or 7 products (279,936) unique feature combinations\n",
    "\n",
    "This will be used to reveal which combination was best in the Gaussian Naive Bayes analysis section\n",
    "\n",
    "index_factor_7products = 400 can change to 10. \n",
    "This is to sample at a rate of one in every 400 for the seven product case\n",
    "'''\n",
    "\n",
    "#FOR 3 PRODUCTS\n",
    "#data_tr = np.vstack((data_array_1_tr,data_array_2_tr,data_array_3_tr)) \n",
    "#data_te = np.vstack((data_array_1_te,data_array_2_te,data_array_3_te)) \n",
    "\n",
    "#FOR 7 PRODUCTS\n",
    "data_tr = np.vstack((data_array_1_tr,data_array_2_tr,data_array_3_tr, data_array_4_tr, data_array_5_tr, data_array_6_tr, data_array_7_tr))\n",
    "data_te = np.vstack((data_array_1_te,data_array_2_te,data_array_3_te, data_array_4_te, data_array_5_te, data_array_6_te, data_array_7_te))\n",
    "\n",
    "\n",
    "\n",
    "# These 0-5 indices correspond, in this order, to the max, min, mean, std, frac, and 0 output by \n",
    "# the 'stats_mbfracdim()' function\n",
    "# if hava added kurtosis and skewness then need to add these lines below following the pattern here\n",
    "\n",
    "data_tr_max = data_tr.T[0]\n",
    "data_tr_min = data_tr.T[1]\n",
    "data_tr_mean = data_tr.T[2]\n",
    "data_tr_std = data_tr.T[3]\n",
    "data_tr_frac = data_tr.T[4]\n",
    "data_tr_zero = data_tr.T[5]\n",
    "\n",
    "ind_nan_frac_tr = np.where(data_tr_frac != data_tr_frac)[0]\n",
    "data_tr_frac[ind_nan_frac_tr] = 0\n",
    "\n",
    "data_te_max = data_te.T[0]\n",
    "data_te_min = data_te.T[1]\n",
    "data_te_mean = data_te.T[2]\n",
    "data_te_std = data_te.T[3]\n",
    "data_te_frac = data_te.T[4]\n",
    "data_te_zero = data_te.T[5]\n",
    "\n",
    "ind_nan_frac_te = np.where(data_te_frac != data_te_frac)[0]\n",
    "data_te_frac[ind_nan_frac_te] = 0\n",
    "\n",
    "\n",
    "\n",
    "product_stats_tr = []\n",
    "product_stats_te = []\n",
    "\n",
    "for l in range(len(name_list)):\n",
    "    product_stats_tr.append((np.array_split(data_tr_max,len(name_list))[l], np.array_split(data_tr_min,len(name_list))[l], np.array_split(data_tr_mean,len(name_list))[l], np.array_split(data_tr_std,len(name_list))[l], np.array_split(data_tr_frac,len(name_list))[l], np.array_split(data_tr_zero,len(name_list))[l] ))\n",
    "    product_stats_te.append((np.array_split(data_te_max,len(name_list))[l], np.array_split(data_te_min,len(name_list))[l], np.array_split(data_te_mean,len(name_list))[l], np.array_split(data_te_std,len(name_list))[l], np.array_split(data_te_frac,len(name_list))[l], np.array_split(data_te_zero,len(name_list))[l] ))\n",
    "\n",
    "\n",
    "### ADJUST IF HAVE 3 OR 7 PRODUCTS\n",
    "product_stats_tr_1 = np.array_split(product_stats_tr, len(name_list))[0][0]\n",
    "product_stats_tr_2 = np.array_split(product_stats_tr, len(name_list))[1][0]\n",
    "product_stats_tr_3 = np.array_split(product_stats_tr, len(name_list))[2][0]\n",
    "product_stats_tr_4 = np.array_split(product_stats_tr, len(name_list))[3][0]\n",
    "product_stats_tr_5 = np.array_split(product_stats_tr, len(name_list))[4][0]\n",
    "product_stats_tr_6 = np.array_split(product_stats_tr, len(name_list))[5][0]\n",
    "product_stats_tr_7 = np.array_split(product_stats_tr, len(name_list))[6][0]\n",
    "\n",
    "\n",
    "product_stats_te_1 = np.array_split(product_stats_te, len(name_list))[0][0]\n",
    "product_stats_te_2 = np.array_split(product_stats_te, len(name_list))[1][0]\n",
    "product_stats_te_3 = np.array_split(product_stats_te, len(name_list))[2][0]\n",
    "product_stats_te_4 = np.array_split(product_stats_te, len(name_list))[3][0]\n",
    "product_stats_te_5 = np.array_split(product_stats_te, len(name_list))[4][0]\n",
    "product_stats_te_6 = np.array_split(product_stats_te, len(name_list))[5][0]\n",
    "product_stats_te_7 = np.array_split(product_stats_te, len(name_list))[6][0]\n",
    "\n",
    "\n",
    "# For 3 products:\n",
    "#product_stats_tr_all_combos = list(itertools.product(product_stats_tr_1,product_stats_tr_2,product_stats_tr_3)) \n",
    "#product_stats_te_all_combos = list(itertools.product(product_stats_te_1,product_stats_te_2,product_stats_te_3)) \n",
    "\n",
    "\n",
    "# Factor at which to subsample the 279936 combinations of one features from each of the 7 products \n",
    "index_factor_7products = 400 #100 #400 #10\n",
    "\n",
    "# For 7 products:\n",
    "product_stats_tr_all_combos = list(itertools.product(product_stats_tr_1,product_stats_tr_2,product_stats_tr_3, product_stats_tr_4, product_stats_tr_5, product_stats_tr_6, product_stats_tr_7))[::index_factor_7products]\n",
    "product_stats_te_all_combos = list(itertools.product(product_stats_te_1,product_stats_te_2,product_stats_te_3, product_stats_te_4, product_stats_te_5, product_stats_te_6, product_stats_te_7))[::index_factor_7products]\n",
    "\n",
    "print('np.shape(product_stats_tr_all_combos):', np.shape(product_stats_tr_all_combos))\n",
    "print('np.shape(product_stats_te_all_combos):', np.shape(product_stats_te_all_combos))\n",
    "\n",
    "\n",
    "\n",
    "#order of max, min, mean, std, frac dim, 0\n",
    "AAA = ['max1','min1', 'mean1', 'std1','frac1', '0']\n",
    "BBB = ['max2','min2', 'mean2', 'std2','frac2', '0']\n",
    "CCC = ['max3','min3', 'mean3', 'std3','frac3', '0']\n",
    "DDD = ['max4','min4', 'mean4', 'std4','frac4', '0']\n",
    "EEE = ['max5','min5', 'mean5', 'std5','frac5', '0']\n",
    "FFF = ['max6','min6', 'mean6', 'std6','frac6', '0']\n",
    "GGG = ['max7','min7', 'mean7', 'std7','frac7', '0']\n",
    "\n",
    "\n",
    "print('np.shape(list(itertools.product(AAA,BBB,CCC))):', np.shape(list(itertools.product(AAA,BBB,CCC))))\n",
    "#print('np.shape(list(itertools.product(AAA,BBB,CCC,DDD,EEE,FFF,GGG))[::index_factor_7products]):', np.shape(list(itertools.product(AAA,BBB,CCC,DDD,EEE,FFF,GGG))[::index_factor_7products]))\n",
    "\n",
    "#for xs in list(itertools.product(AAA,BBB,CCC)): #,DDD,EEE,FFF,GGG)): #[-100:]:\n",
    "#    print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*** SKIP IF 3 or 7 PRODUCTS\n",
    "\n",
    "USE ONLY IF HAVE 1 PRODUCT\n",
    "\n",
    "Generates all combinations of the max, min, mean, std, and frac features for MDI only for 31 unique \n",
    "feature combinations. This will be used to reveal which combination was best in the Gaussian Naive Bayes analysis section\n",
    "'''\n",
    "\n",
    "lst = ['max','min','mean','std','frac']\n",
    "combs = []\n",
    "\n",
    "for i in range(1, len(lst)+1):\n",
    "    els = [list(x) for x in itertools.combinations(lst, i)]\n",
    "    combs.extend(els)\n",
    "\n",
    "print(np.shape(combs))\n",
    "\n",
    "for combi in combs:\n",
    "    print(combi)\n",
    "\n",
    "    \n",
    "\n",
    "#since don't need the end zero in this case which is the 6th row\n",
    "one_prod_tr = np.array(data_array_1_tr).T[0:5] \n",
    "one_prod_te = np.array(data_array_1_te).T[0:5] \n",
    "\n",
    "print('np.shape(one_prod_tr):', np.shape(one_prod_tr))\n",
    "print('np.shape(one_prod_te):', np.shape(one_prod_te))\n",
    "\n",
    "product_stats_tr_all_combos_pre = []\n",
    "product_stats_te_all_combos_pre = []\n",
    "\n",
    "for i in range(1, len(one_prod_tr)+1):\n",
    "    combs_tr = [np.array(x) for x in itertools.combinations(one_prod_tr, i)]\n",
    "    product_stats_tr_all_combos_pre.extend(combs_tr)\n",
    "\n",
    "for i in range(1, len(one_prod_te)+1):\n",
    "    combs_te = [np.array(x) for x in itertools.combinations(one_prod_te, i)]\n",
    "    product_stats_te_all_combos_pre.extend(combs_te)\n",
    "\n",
    "\n",
    "product_stats_tr_all_combos = []  \n",
    "product_stats_te_all_combos = []\n",
    "\n",
    "for combi in product_stats_tr_all_combos_pre:\n",
    "    product_stats_tr_all_combos.append(np.array_split(np.ravel(combi, order='F'),Bz_train_set_len))\n",
    "\n",
    "\n",
    "for combi in product_stats_te_all_combos_pre:\n",
    "    product_stats_te_all_combos.append(np.array_split(np.ravel(combi, order='F'),Bz_test_set_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the order of the SOHO Products for each experiment\n",
    "It is this order which determines the singleton features found\n",
    "'''\n",
    "\n",
    "print('product_name_list:', product_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian Naive Bayes Analysis\n",
    "TSS and MCC scores per threshold\n",
    "\n",
    "NEED TO ADJUST FOR 1,3, OR 7 PRODUCTS in six places!\n",
    "'''\n",
    "\n",
    "GNB_MCC_on_test_all5thres = []\n",
    "GNB_TSS_on_test_all5thres = []\n",
    "tfnp = []\n",
    "\n",
    "for thres in range(len(Bz_thresholds_on_train_MDIonly)):\n",
    "    GNB_MCC_on_test_all5thres_local = []\n",
    "    GNB_TSS_on_test_all5thres_local = []\n",
    "    tfnp_local = []\n",
    "    \n",
    "    class_weights_prep = np.ones(Bz_train_set_len)\n",
    "    class_weights_prep[np.where(per_threshold_given_products_train_labels[thres] == 0)[0]] = class_weights_list[thres][0]\n",
    "    class_weights_prep[np.where(per_threshold_given_products_train_labels[thres] == 1)[0]] = class_weights_list[thres][1]    \n",
    "    \n",
    "    for l,combo in enumerate(product_stats_tr_all_combos):\n",
    "        \n",
    "        # For 1 product:\n",
    "        #combo_zip = combo\n",
    "        \n",
    "        # For 3 products:\n",
    "        #combo_zip = list(zip(combo[0],combo[1],combo[2])) \n",
    "        \n",
    "        # For 7 products:\n",
    "        combo_zip = list(zip(combo[0],combo[1],combo[2],combo[3],combo[4],combo[5],combo[6]))\n",
    "        \n",
    "        clf_GaussianNB = GaussianNB()\n",
    "        clf_GaussianNB.fit(combo_zip, per_threshold_given_products_train_labels[thres], class_weights_prep)\n",
    "\n",
    "        # For 1 product:\n",
    "        #combo_zip_test = product_stats_te_all_combos[l] #for MDI (31, 1227)\n",
    "        \n",
    "        # For 3 products:\n",
    "        #combo_zip_test = list(zip(product_stats_te_all_combos[l][0],product_stats_te_all_combos[l][1],product_stats_te_all_combos[l][2])) \n",
    "        \n",
    "        # For 7 products:\n",
    "        combo_zip_test = list(zip(product_stats_te_all_combos[l][0],product_stats_te_all_combos[l][1],product_stats_te_all_combos[l][2],product_stats_te_all_combos[l][3],product_stats_te_all_combos[l][4],product_stats_te_all_combos[l][5],product_stats_te_all_combos[l][6]))\n",
    "        \n",
    "        GNB_pred_on_test = clf_GaussianNB.predict(combo_zip_test)\n",
    "        GNB_matthews_on_test = matthews_corrcoef(per_threshold_given_products_test_labels[thres],GNB_pred_on_test)\n",
    "        tn_g, fp_g, fn_g, tp_g = confusion_matrix(per_threshold_given_products_test_labels[thres],GNB_pred_on_test).ravel()\n",
    "        tfnp.append([tn_g, fp_g, fn_g, tp_g])\n",
    "        GNB_TSS_on_test = tp_g/(tp_g + fn_g) - fp_g/(fp_g + tn_g)\n",
    "        #GNB_matthews_on_test = ( (tp_g*tn_g)-(fp_g*fn_g) ) / np.sqrt((tp_g + fp_g)*(tp_g + fn_g)*(tn_g + fp_g)*(tn_g + fn_g)) #manual version\n",
    "        #print('GNB_matthews_on_test:', GNB_matthews_on_test)\n",
    "        #if GNB_matthews_on_test != GNB_matthews_on_test:\n",
    "        #    GNB_matthews_on_test = 0\n",
    "        GNB_MCC_on_test_all5thres.append(GNB_matthews_on_test)\n",
    "        GNB_TSS_on_test_all5thres.append(GNB_TSS_on_test)\n",
    "        \n",
    "        GNB_MCC_on_test_all5thres_local.append(GNB_matthews_on_test)\n",
    "        GNB_TSS_on_test_all5thres_local.append(GNB_TSS_on_test)\n",
    "        \n",
    "        tfnp_local.append([tn_g, fp_g, fn_g, tp_g])\n",
    "    \n",
    "    ###\n",
    "    print(f'threshold{thres+1}')\n",
    "    print('np.unique(class_weights_prep):', np.unique(class_weights_prep))\n",
    "    print('\\n')\n",
    "    \n",
    "    ###\n",
    "    GNB_MCC_on_test_all5thres_local_max = np.max(GNB_MCC_on_test_all5thres_local)\n",
    "    print('GNB_MCC_on_test_all5thres_local_max:', GNB_MCC_on_test_all5thres_local_max)\n",
    "    ind_MCC_on_test_max_local = np.where(GNB_MCC_on_test_all5thres_local == GNB_MCC_on_test_all5thres_local_max)[0][0]\n",
    "    print('ind_MCC_on_test_max_local:', ind_MCC_on_test_max_local)\n",
    "    print('tn_g, fp_g, fn_g, tp_g tfnp_local[ind_MCC_on_test_max_local]:', tfnp_local[ind_MCC_on_test_max_local])\n",
    "    \n",
    "    ### *** NEED TO UPDATE THIS LINE IF 1, 3, OR 7 PRODUCTS *** ###\n",
    "    \n",
    "    # For 1 product:\n",
    "    #print('combs[ind_MCC_on_test_max_local]:', combs[ind_MCC_on_test_max_local])\n",
    "    \n",
    "    ### *** NEED TO CHANGE BETWEEN 3 AND 7 PRODUCTS *** ###\n",
    "    \n",
    "    # For 3 products:\n",
    "    #print(list(itertools.product(AAA,BBB,CCC))[ind_MCC_on_test_max_local])\n",
    "    \n",
    "    # For 7 products:\n",
    "    print(list(itertools.product(AAA,BBB,CCC,DDD,EEE,FFF,GGG))[::index_factor_7products][ind_MCC_on_test_max_local]) \n",
    "\n",
    "    \n",
    "    print('TSS for MCC global max local:', GNB_TSS_on_test_all5thres_local[ind_MCC_on_test_max_local])\n",
    "    \n",
    "    ###\n",
    "    GNB_TSS_on_test_all5thres_local_max = np.max(GNB_TSS_on_test_all5thres_local)\n",
    "    print('GNB_TSS_on_test_all5thres_local_max:', GNB_TSS_on_test_all5thres_local_max)\n",
    "    ind_TSS_on_test_max_local = np.where(GNB_TSS_on_test_all5thres_local == GNB_TSS_on_test_all5thres_local_max)[0][0]\n",
    "    print('ind_TSS_on_test_max_local:', ind_TSS_on_test_max_local)\n",
    "    print('tn_g, fp_g, fn_g, tp_g tfnp_local[ind_TSS_on_test_max_local]:', tfnp_local[ind_TSS_on_test_max_local])    \n",
    "    \n",
    "    \n",
    "    ### *** NEED TO UPDATE THIS LINE IF 1, 3, OR 7 PRODUCTS *** ###\n",
    "    \n",
    "    # For 1 product:\n",
    "    #print('combs[ind_TSS_on_test_max_local]:', combs[ind_TSS_on_test_max_local])\n",
    "    \n",
    "    \n",
    "    ### *** NEED TO CHANGE BETWEEN 3 AND 7 PRODUCTS *** ###\n",
    "    \n",
    "    # For 3 products:\n",
    "    #print(list(itertools.product(AAA,BBB,CCC))[ind_TSS_on_test_max_local])\n",
    "    \n",
    "    # For 7 products:\n",
    "    print(list(itertools.product(AAA,BBB,CCC,DDD,EEE,FFF,GGG))[::index_factor_7products][ind_TSS_on_test_max_local])\n",
    "    \n",
    "    print('MCC for TSS global max local:', GNB_MCC_on_test_all5thres_local[ind_TSS_on_test_max_local])\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    print(f'MCC max on threshold{thres+1}:', GNB_MCC_on_test_all5thres_local_max)\n",
    "    print(f'TSS max on threshold{thres+1}:', GNB_TSS_on_test_all5thres_local_max)\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "print('\\n')\n",
    "GNB_MCC_on_test_all5thres_max = np.max(GNB_MCC_on_test_all5thres)\n",
    "print('global max MCC:', GNB_MCC_on_test_all5thres_max)\n",
    "ind_MCC_on_test_max = np.where(GNB_MCC_on_test_all5thres == GNB_MCC_on_test_all5thres_max)[0][0] % len(product_stats_tr_all_combos)\n",
    "print('ind_MCC_on_test_max:', ind_MCC_on_test_max)\n",
    "print('tn_g, fp_g, fn_g, tp_g tfnp[ind_MCC_on_test_max]:', tfnp[ind_MCC_on_test_max])\n",
    "\n",
    "\n",
    "### *** NEED TO UPDATE THIS LINE IF 1, 3, OR 7 PRODUCTS *** ###\n",
    "\n",
    "# For 1 product:\n",
    "#print('combs[ind_MCC_on_test_max]:', combs[ind_MCC_on_test_max])\n",
    "\n",
    "\n",
    "### *** NEED TO CHANGE BETWEEN 3 AND 7 PRODUCTS *** ###\n",
    "\n",
    "# For 3 products:\n",
    "#print(list(itertools.product(AAA,BBB,CCC))[ind_MCC_on_test_max])\n",
    "\n",
    "# For 7 products:\n",
    "print(list(itertools.product(AAA,BBB,CCC,DDD,EEE,FFF,GGG))[::index_factor_7products][ind_MCC_on_test_max])\n",
    "\n",
    "print('TSS for MCC global max:', GNB_TSS_on_test_all5thres[ind_MCC_on_test_max])\n",
    "\n",
    "print('\\n')\n",
    "GNB_TSS_on_test_all5thres_max = np.max(GNB_TSS_on_test_all5thres)\n",
    "print('global max TSS:', GNB_TSS_on_test_all5thres_max)\n",
    "ind_TSS_on_test_max = np.where(GNB_TSS_on_test_all5thres == GNB_TSS_on_test_all5thres_max)[0][0] % len(product_stats_tr_all_combos)\n",
    "print('ind_TSS_on_test_max:', ind_TSS_on_test_max)\n",
    "print('tn_g, fp_g, fn_g, tp_g tfnp[ind_TSS_on_test_max]:', tfnp[ind_TSS_on_test_max])\n",
    "\n",
    "### *** NEED TO UPDATE THIS LINE IF 1, 3, OR 7 PRODUCTS *** ###\n",
    "\n",
    "# For 1 product:\n",
    "#print('combs[ind_TSS_on_test_max]:', combs[ind_TSS_on_test_max])\n",
    "\n",
    "### *** NEED TO CHANGE BETWEEN 3 AND 7 PRODUCTS *** ###\n",
    "\n",
    "# For 3 products\n",
    "#print(list(itertools.product(AAA,BBB,CCC))[ind_TSS_on_test_max])\n",
    "\n",
    "# For 7 products\n",
    "print(list(itertools.product(AAA,BBB,CCC,DDD,EEE,FFF,GGG))[::index_factor_7products][ind_TSS_on_test_max])\n",
    "\n",
    "print('MCC for TSS global max:', GNB_MCC_on_test_all5thres[ind_TSS_on_test_max])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}