{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "from skimage.transform import rescale\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import parser\n",
    "import time\n",
    "from time import process_time\n",
    "\n",
    "from sunpy.net import Fido\n",
    "from sunpy.net.vso import attrs as avso\n",
    "from sunpy.time import TimeRange #parse_time,\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "\n",
    "import h5py\n",
    "\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfits(filename):\n",
    "    try:\n",
    "        ft = fits.open(filename, memmap=False)\n",
    "        hdr = ft[0].header\n",
    "        data = ft[0].data\n",
    "        axis1 = hdr['naxis1']\n",
    "        axis2 = hdr['naxis2']\n",
    "        axisnum = hdr['naxis']\n",
    "        ft.close()\n",
    "    \n",
    "    except ValueError:\n",
    "        axis1 = 1\n",
    "        axis2 = 2\n",
    "        axisnum = 3\n",
    "        data = None\n",
    "            \n",
    "    return axis1,axis2,data,axisnum\n",
    "\n",
    "def writefits(filename, data, home_dir):\n",
    "    if not os.path.exists(f'{home_dir}{filename}.fits'):\n",
    "        fitsname = fits.PrimaryHDU(data)\n",
    "        fitsname.writeto(f'{home_dir}{filename}.fits')\n",
    "\n",
    "def holes(filename):\n",
    "    filename = str(filename)\n",
    "    \n",
    "    ft = fits.open(filename, memmap=False)\n",
    "    hdr = ft[0].header\n",
    "    data = ft[0].data\n",
    "    ft.close()\n",
    "\n",
    "    try:\n",
    "        x_coord = hdr['CRPIX1']\n",
    "        y_coord = hdr['CRPIX2']\n",
    "    \n",
    "    except KeyError:\n",
    "        x_coord = hdr['naxis1'] / 2.\n",
    "        y_coord = hdr['naxis2'] / 2.\n",
    "\n",
    "    y_ind,x_ind = np.indices((hdr['naxis1'],hdr['naxis2']))\n",
    "    rsquared = (x_ind - x_coord)**2 + (y_ind - y_coord)**2\n",
    "    \n",
    "    matches = ['96m', 'MDI']\n",
    "    \n",
    "    if 'efz' in filename: #good for all EIT products \n",
    "        rad = x_coord*np.sqrt(2)\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "\n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "    \n",
    "    elif any([x in filename for x in matches]):\n",
    "        rad1 = float(x_coord)\n",
    "        rad2 = 0.6*float(x_coord)\n",
    "        indices_rad1 = np.where(rsquared.flatten() < rad1**2)[0]\n",
    "        indices_rad2 = np.where(rsquared.flatten() < rad2**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices_rad1] == 0.)[0]\n",
    "        nan_ind = np.where(data.flatten()[indices_rad2] != data.flatten()[indices_rad2])[0]\n",
    "        zeros_nan_ind_len = len(list(zeros_ind) + list(nan_ind))\n",
    "        \n",
    "        if zeros_nan_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "\n",
    "    elif 'LASCO_C3' in filename:\n",
    "        #print('LASCO_C3')\n",
    "        rad = 0.8*x_coord\n",
    "        indices = np.where(rsquared.flatten() < rad**2)[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)  \n",
    "        \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image   \n",
    "\n",
    "    \n",
    "    elif 'LASCO_C2' in filename:\n",
    "        #print('LASCO_C2')\n",
    "        rad1 = 160 #this seems good\n",
    "        #print('rad1:', rad1)\n",
    "        rad2 = int(x_coord)\n",
    "        indices = np.where((rad2**2 > rsquared.flatten()) & (rsquared.flatten() > rad1**2))[0]\n",
    "        zeros_ind = np.where(data.flatten()[indices] == 0.)[0]\n",
    "        zeros_ind_len = len(zeros_ind)\n",
    "     \n",
    "        if zeros_ind_len > 100:\n",
    "            return True #so image not useable as there are holes\n",
    "        else:\n",
    "            return False #can use this image\n",
    "        \n",
    "\n",
    "def data_reducer(data,flag,target_dimension,axis1_shape):\n",
    "    scale_factor = int(axis1_shape/target_dimension)\n",
    "    \n",
    "    if flag == 'subsample':\n",
    "        reduced_data = data[::scale_factor].T[::scale_factor].T #subsampling image; every other row,column\n",
    "    elif flag == 'interp': #linear interpolation with anti_aliasing and range preserving\n",
    "        reduced_data = rescale(data, (1/scale_factor), order=1, anti_aliasing=True, preserve_range=True)\n",
    "    elif flag == 'minpool': #min pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.min)\n",
    "    elif flag == 'maxpool': #max pooling each block\n",
    "        reduced_data = block_reduce(data, block_size=(scale_factor,scale_factor), func=np.max)\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "\n",
    "def prev_time_resumer(home_dir, base, time_range_orig, date_time_end): \n",
    "    #CAN RE-RUN PROGRAM FROM THE LAST DATE ON WHICH STOPPED; WILL PICK UP THE TIMES THAT ARE PRESENT AND CONTINUE!\n",
    "    # CHECKS WHETHER THE START DAY THAT ENTERED IS ALREADY CONTAINED IN THE FILES OF PREVIOUS DAY AND START_DATE FROM THAT EXACT TIME! \n",
    "    # ALSO THIS WORKS IF START ON A NEW DAY AND ARE LOOKING BACK ON THE PREVIOUS DAY.\n",
    "    \n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre)\n",
    "    \n",
    "    if len(data_files) != 0:\n",
    "        prev_time_pre = data_files[-1]\n",
    "        if 'EIT' in str(prev_time_pre):\n",
    "            prev_time = [prev_time_pre.split('_')[2]]\n",
    "        else:\n",
    "            prev_time = [prev_time_pre.split('_')[3]]  \n",
    "            \n",
    "        time_orig_pre = str(time_range_orig.start)\n",
    "        time_orig = ''.join(time_orig_pre.split(' ')[0].split('-'))\n",
    "        \n",
    "        if str(prev_time[0][0:8]) == time_orig:\n",
    "            time_begin = prev_time[0]\n",
    "            time_range = TimeRange(time_begin, date_time_end)\n",
    "        else:\n",
    "            time_range = time_range_orig            \n",
    "    \n",
    "    elif len(data_files) == 0:\n",
    "        prev_time = []\n",
    "        time_range = time_range_orig   \n",
    "    \n",
    "    return prev_time, time_range\n",
    "\n",
    "\n",
    "def date_name_maker(date_name):\n",
    "\n",
    "    date_name_chunks = [date_name[i:i+2] for i in range(0,len(date_name),2)]\n",
    "    date_name_new = ''.join(date_name_chunks[0:2])+'-'+'-'.join(date_name_chunks[2:4])+'-'+':'.join(date_name_chunks[4:7])\n",
    "    \n",
    "    return date_name_new\n",
    "\n",
    "\n",
    "def data_name_selector(home_dir, base, date_start, date_finish):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre)\n",
    "    print('len(data_files):', len(data_files)) \n",
    "    \n",
    "    if len(data_files) != 0: \n",
    "        time_start_name_pre = data_files[0] \n",
    "        time_finish_name_pre = data_files[-1]\n",
    "        \n",
    "        if 'EIT' in str(time_start_name_pre):\n",
    "            time_start_name = str(time_start_name_pre.split('_')[2])\n",
    "            time_finish_name = str(time_finish_name_pre.split('_')[2])        \n",
    "        else:         \n",
    "            time_start_name = str(time_start_name_pre.split('_')[3])\n",
    "            time_finish_name = str(time_finish_name_pre.split('_')[3])\n",
    "            \n",
    "        time_start_name_new = date_name_maker(time_start_name)\n",
    "        time_finish_name_new = date_name_maker(time_finish_name)\n",
    "    \n",
    "    else: \n",
    "        time_start_name_new = date_start \n",
    "        time_finish_name_new = date_finish  \n",
    "        \n",
    "    return time_start_name_new, time_finish_name_new\n",
    "\n",
    "\n",
    "def data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension):\n",
    "\n",
    "    print('base:', base)\n",
    "    filepath = home_dir + base + '/'\n",
    "\n",
    "    data_files_pre = [f for f in listdir(filepath) if isfile(join(filepath, f))]\n",
    "    data_files = np.sort(data_files_pre) #to have chronological order and to sink order with list of individual product times\n",
    "    print('len(data_files):', len(data_files))\n",
    "    \n",
    "    data_content_list = []\n",
    "    for elem in data_files:\n",
    "        axdim1,axdim2,data_content,axisnum = readfits(f'{filepath}{elem}')\n",
    "        if 'SOHO' in elem:\n",
    "            data_content_list.append(data_content)\n",
    "\n",
    "    if data_content_list:\n",
    "        data_content_stack = np.stack(data_content_list)\n",
    "    else:\n",
    "        data_content_stack = []\n",
    "    \n",
    "    time_start_name_new, time_finish_name_new = data_name_selector(home_dir, base, date_start, date_finish)\n",
    "    \n",
    "    data_content_stack = np.stack(data_content_list)\n",
    "    data_cube = h5py.File(f'{home_dir}{time_start_name_new}_to_{time_finish_name_new}_{base}_{flag}_{target_dimension}.h5', 'w')\n",
    "    data_cube.create_dataset(f'{base}_{target_dimension}', data=data_content_stack, compression=\"gzip\")\n",
    "    data_cube.close()\n",
    "                            \n",
    "    return data_cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_search(base,time_range,date_time_start):\n",
    "    if 'EIT' in base:\n",
    "        wavelen = int(base[3:6])\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('EIT'),avso.Provider('SDAC'),avso.Wavelength(wavelen * avso.u.Angstrom, wavelen * avso.u.Angstrom))\n",
    "    \n",
    "    elif 'MDI' in base:\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Source('SOHO'),avso.Instrument('MDI'),avso.Provider('SDAC'),avso.Physobs('LOS_MAGNETIC_FIELD'))\n",
    "    \n",
    "    elif 'LASCO' in base:\n",
    "        detector = base.split('_')[1]\n",
    "        product_results = Fido.search(avso.Time(time_range,date_time_start),avso.Provider('SDAC'),avso.Source('SOHO'),avso.Instrument('LASCO'),avso.Detector(detector))\n",
    "    \n",
    "    return product_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_sizes(base,product_results):\n",
    "    \n",
    "    matches = ['171', '304', '284']\n",
    "    \n",
    "    if 'EIT195' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind_2059 = np.where(np.array(size_list) == 2059)[0]\n",
    "        ind_523 = np.where(np.array(size_list) == 523)[0]\n",
    "        print(len(ind_2059))\n",
    "        print(len(ind_523))\n",
    "        ind = np.sort(list(ind_2059) + list(ind_523)) #important to sort here since combining two lists!\n",
    "        print(len(ind))\n",
    "        \n",
    "    elif 'MDI' in base:\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 4115.0)[0]\n",
    "        print(len(ind))        \n",
    "        \n",
    "    elif 'LASCO' in base:\n",
    "        size_list = [int(np.ceil(elem['size'] / 100.0))*100 for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2100.0)[0] \n",
    "        print(len(ind))\n",
    "        \n",
    "    elif any([x in base for x in matches]):\n",
    "        size_list = [elem['size'] for elem in product_results.get_response(0)[:]]\n",
    "        print(np.unique(size_list), len(size_list))\n",
    "        ind = np.where(np.array(size_list) == 2059)[0]        \n",
    "        print(len(ind))\n",
    "        \n",
    "    return ind\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_indices(base,ind,product_results,time_window,look_ahead,prev_time):\n",
    "    \n",
    "    all_size_sieved_times_pre = [] #local list to populate at each loop\n",
    "    all_time_window_sieved_times_product_times = []  #local list to populate at each loop\n",
    "\n",
    "    for value in ind:\n",
    "        all_size_sieved_times_pre.append(product_results.get_response(0)[int(value)]['time']['start'])\n",
    "    all_size_sieved_times = list(np.unique(all_size_sieved_times_pre))\n",
    "    all_size_sieved_times_aug = prev_time + all_size_sieved_times #prev_time = [] for the very first loop and [last best time from previous loop] for subsequent loops.\n",
    "\n",
    "    if all_size_sieved_times_aug:\n",
    "        if prev_time:\n",
    "            local_time_range = TimeRange(all_size_sieved_times_aug[0],timedelta(hours=time_window)).next() #next() is the important difference here.\n",
    "        else:\n",
    "            local_time_range = TimeRange(all_size_sieved_times_aug[0],timedelta(hours=time_window))       \n",
    "        for i,time_value in enumerate(all_size_sieved_times_aug):\n",
    "            if time_value in local_time_range:\n",
    "                all_time_window_sieved_times_product_times.append(time_value)\n",
    "                local_time_range = TimeRange(time_value,timedelta(hours=time_window)).next() #important distinction between this local_time_range and the intializing one is the presence of time_value          \n",
    "            elif parser.parse(time_value) > local_time_range.end: \n",
    "                local_time_range = TimeRange(time_value,timedelta(hours=time_window)) #important not to have next here.\n",
    "            else:\n",
    "                continue \n",
    "                \n",
    "    new_inds = [np.where(np.array(all_size_sieved_times_pre) == entry)[0][0] for entry in all_time_window_sieved_times_product_times]\n",
    "    fetch_indices_product = ind[new_inds]\n",
    "    \n",
    "    return all_size_sieved_times_pre, fetch_indices_product\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_retriever(base,product_results,indiv_ind,url_prefix,home_dir):\n",
    "    \n",
    "    fileid = product_results.get_response(0)[int(indiv_ind)]['fileid']\n",
    "    item_wget =  url_prefix + fileid\n",
    "    cmd = 'wget' + ' ' + '--retry-connrefused' + ' ' + '--waitretry=1' + ' ' + '--read-timeout=20' + ' ' + '--timeout=15' + ' ' + '-t' + ' ' + '0' + ' ' + '--continue' + ' ' + item_wget + ' ' + '-P' + ' ' + f'{home_dir}{base}'     \n",
    "    args = shlex.split(cmd)    \n",
    "    \n",
    "    try: \n",
    "        wget_output = subprocess.check_output(args, stderr=subprocess.STDOUT)\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        print('Error output:\\n', err.output) \n",
    "        print('sleep for 15 minutes and then retry command')\n",
    "        time.sleep(900)\n",
    "        wget_output = subprocess.check_output(args, stderr=subprocess.STDOUT)\n",
    "        \n",
    "    downloaded_fileid = fileid.split('/')[-1]\n",
    "    query_result = [f'{home_dir}{base}/{downloaded_fileid}']\n",
    "    \n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_distiller(fetch_indices_product_orig, base, all_size_sieved_times_pre, ind, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir):\n",
    "    \n",
    "    holes_product_list = []\n",
    "    unreadable_file_ids_product_list = []\n",
    "    \n",
    "    all_time_window_sieved_times_product_times = []\n",
    "    all_time_window_sieved_times_product_times_inds_list = []\n",
    "    \n",
    "    fetch_indices_product = fetch_indices_product_orig.copy()\n",
    "    \n",
    "    for i,elem in enumerate(fetch_indices_product):\n",
    "        if (i > len(fetch_indices_product)-1): #fetch_indices_product is modified in the program to account for times corresponding to holes in images. When all fitting times exhausted then break out of loop.\n",
    "            break         \n",
    "        indiv_ind = fetch_indices_product[i]\n",
    "        query_result = product_retriever(base,product_results,indiv_ind,url_prefix,home_dir) #item -> indiv_ind is a member of fetch_indices_product\n",
    "        axis1_product, axis2_product, data_product, axisnum_product = readfits(query_result[0])\n",
    "    \n",
    "        if (data_product is not None) and (axis1_product == axis2_product) and (axisnum_product == 2):\n",
    "\n",
    "            if not holes(query_result[0]): #so if not True; so no holes; can use image\n",
    "                reduced_product_data = data_reducer(data_product,flag,target_dimension,axis1_product)\n",
    "                time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "                writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "                os.remove(query_result[0]) #delete original downloaded file\n",
    "                all_time_window_sieved_times_product_times.append(time_data)\n",
    "                all_time_window_sieved_times_product_times_inds_list.append(indiv_ind)\n",
    "\n",
    "            elif holes(query_result[0]): #so if True, if there are holes\n",
    "                time_data = product_results.get_response(0)[int(indiv_ind)]['time']['start'] \n",
    "                hole_loc = url_prefix + product_results.get_response(0)[int(indiv_ind)]['fileid']                       \n",
    "                holes_product_list.append((hole_loc, str(time_data)))\n",
    "                hole_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "                os.remove(query_result[0]) #delete original downloaded file\n",
    "                ind_timespickup = np.where(np.array(all_size_sieved_times_pre) == hole_time_val)[0][0]\n",
    "                zoomed_time_range = TimeRange(str(hole_time_val),timedelta(hours=time_window))\n",
    "\n",
    "                fetch_inds_to_try_list = [] \n",
    "                #the zeroth entry didn't have it so that's why plus 1 in the brackets\n",
    "                for time_val in all_size_sieved_times_pre[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "                    if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                        ind_next_good_time = np.where(np.array(all_size_sieved_times_pre) == time_val)[0][0]\n",
    "                        fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                        fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "                for index in fetch_inds_to_try_list:\n",
    "                    query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "                    axis1_next_good,axis2_next_good,data_next_good,axisnum_next_good = readfits(query_result_next[0])\n",
    "\n",
    "                    if (data_next_good is not None) and (axis1_next_good == axis2_next_good) and (axisnum_next_good == 2):\n",
    "\n",
    "                        if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                            reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                            time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                            writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                            all_time_window_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                            all_time_window_sieved_times_product_times_inds_list.append(index)\n",
    "                            os.remove(query_result_next[0]) #delete original downloaded file\n",
    "                            \n",
    "                            indiv_ind_modified_list = []\n",
    "                            localized_time_range = TimeRange(str(time_data),timedelta(hours=time_window)).next() \n",
    "                            print('localized_time_range:', localized_time_range)\n",
    "                            for tval in all_size_sieved_times_pre: \n",
    "                                if parser.parse(tval) < localized_time_range.start:\n",
    "                                    continue \n",
    "                                elif tval in localized_time_range:\n",
    "                                    ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                    indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                    indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                    localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()                                \n",
    "                                else:\n",
    "                                    ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                    indiv_ind_modified_new = ind[ind_time_new]            \n",
    "                                    next_orig_index = np.where(np.array(fetch_indices_product_orig) == np.array(indiv_ind_modified_new))[0]\n",
    "                                    if len(next_orig_index) != 0:\n",
    "                                        indiv_ind_modified_new = fetch_indices_product_orig[next_orig_index[0]]\n",
    "                                        ind_next_index = np.where(np.array(ind) == indiv_ind_modified_new)[0][0]\n",
    "                                        tval = all_size_sieved_times_pre[ind_next_index]\n",
    "                                        indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                        localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "                                    else:\n",
    "                                        ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                        indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                        indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                        localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "                                    \n",
    "                            print('indiv_ind_modified_list:', indiv_ind_modified_list)\n",
    "                            \n",
    "                            if indiv_ind_modified_list:\n",
    "                                fetch_indices_product = list(np.zeros(i+1)) + list(indiv_ind_modified_list) #trick to add zeros to maintain same length as original fetch_indices_product\n",
    "                            else:\n",
    "                                fetch_indices_product = list(np.zeros(i+1))\n",
    "                            break\n",
    "\n",
    "                        elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                            time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                            hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                            holes_product_list.append((hole_loc, str(time_data)))\n",
    "                            os.remove(query_result_next[0])\n",
    "                            continue \n",
    "\n",
    "                    elif (data_next_good is None) or (axis1_next_good != axis2_next_good) or (axisnum_next_good != 2):\n",
    "                        unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                        os.remove(query_result_next[0])\n",
    "                        continue\n",
    "\n",
    "\n",
    "        elif (data_product is None) or (axis1_product != axis2_product) or (axisnum_product != 2):\n",
    "            unreadable_file_ids_product_list.append(product_results.get_response(0)[int(indiv_ind)]['fileid'])\n",
    "            bad_time_val = product_results.get_response(0)[int(indiv_ind)]['time']['start']\n",
    "            os.remove(query_result[0])\n",
    "            ind_timespickup = np.where(np.array(all_size_sieved_times_pre) == bad_time_val)[0][0]\n",
    "            zoomed_time_range = TimeRange(str(bad_time_val),timedelta(hours=time_window))\n",
    "\n",
    "            fetch_inds_to_try_list = [] #gets reset for each new item\n",
    "            for time_val in all_size_sieved_times_pre[ind_timespickup+1: ind_timespickup + look_ahead]:\n",
    "                if time_val in zoomed_time_range: #this is the next fitting time in the list, slightly less than 2hrs seperated theoretically\n",
    "                    ind_next_good_time = np.where(np.array(all_size_sieved_times_pre) == time_val)[0][0]\n",
    "                    fetch_indices_next_good = ind[ind_next_good_time]\n",
    "                    fetch_inds_to_try_list.append(fetch_indices_next_good)\n",
    "\n",
    "            for index in fetch_inds_to_try_list:\n",
    "                query_result_next = product_retriever(base,product_results,index,url_prefix,home_dir)\n",
    "                axis1_next_good,axis2_next_good,data_next_good, axisnum_next_good = readfits(query_result_next[0])\n",
    "\n",
    "                if (data_next_good is not None) and (axis1_next_good == axis2_next_good) and (axisnum_next_good == 2):\n",
    "\n",
    "                    if not holes(query_result_next[0]): #so if not True; so no holes; can use image\n",
    "                        reduced_product_data = data_reducer(data_next_good,flag,target_dimension,axis1_next_good)\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        writefits(f'{base}/SOHO_{base}_{time_data}_{target_dimension}', reduced_product_data, home_dir)\n",
    "\n",
    "                        all_time_window_sieved_times_product_times.append(time_data) #(time_val) #unsorted time location\n",
    "                        all_time_window_sieved_times_product_times_inds_list.append(index)\n",
    "                        os.remove(query_result_next[0])\n",
    "                        \n",
    "                        indiv_ind_modified_list = []\n",
    "                        localized_time_range = TimeRange(str(time_data),timedelta(hours=time_window)).next()\n",
    "                        for tval in all_size_sieved_times_pre:\n",
    "                            if parser.parse(tval) < localized_time_range.start:\n",
    "                                continue \n",
    "                            elif tval in localized_time_range:\n",
    "                                ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()                                \n",
    "                            else:\n",
    "                                ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                indiv_ind_modified_new = ind[ind_time_new]   \n",
    "                                next_orig_index = np.where(np.array(fetch_indices_product_orig) == np.array(indiv_ind_modified_new))[0]\n",
    "                                if len(next_orig_index) != 0:\n",
    "                                    indiv_ind_modified_new = fetch_indices_product_orig[next_orig_index[0]]\n",
    "                                    ind_next_index = np.where(np.array(ind) == indiv_ind_modified_new)[0][0]\n",
    "                                    tval = all_size_sieved_times_pre[ind_next_index]\n",
    "                                    indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                    localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "                                else:\n",
    "                                    ind_time_new = np.where(np.array(all_size_sieved_times_pre) == tval)[0][0]\n",
    "                                    indiv_ind_modified_new = ind[ind_time_new]\n",
    "                                    indiv_ind_modified_list.append(indiv_ind_modified_new)\n",
    "                                    localized_time_range = TimeRange(str(tval),timedelta(hours=time_window)).next()\n",
    "\n",
    "                        print('indiv_ind_modified_list:', indiv_ind_modified_list)\n",
    "                        \n",
    "                        if indiv_ind_modified_list:\n",
    "                            fetch_indices_product = list(np.zeros(i+1)) + list(indiv_ind_modified_list)\n",
    "                        else:\n",
    "                            fetch_indices_product = list(np.zeros(i+1))\n",
    "                        break\n",
    "\n",
    "                    elif holes(query_result_next[0]): #so if True, if there are holes\n",
    "                        time_data = product_results.get_response(0)[int(index)]['time']['start']\n",
    "                        hole_loc = url_prefix + product_results.get_response(0)[int(index)]['fileid']\n",
    "                        holes_product_list.append((hole_loc, str(time_data)))\n",
    "                        os.remove(query_result_next[0])\n",
    "                        continue \n",
    "\n",
    "                elif (data_next_good is None) or (axis1_next_good != axis2_next_good) or (axisnum_next_good != 2):\n",
    "                    unreadable_file_ids_product_list.append(product_results.get_response(0)[int(index)]['fileid'])\n",
    "                    os.remove(query_result_next[0])\n",
    "                    continue\n",
    "    \n",
    "    all_time_window_sieved_times_product_times_modified = all_time_window_sieved_times_product_times\n",
    "\n",
    "    return all_time_window_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keeps first track of all times of all fits files gathered.\n",
    "\"\"\"\n",
    "def csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_time_window_sieved_times_sorted):\n",
    "    with open(f'{home_dir}{date_start}_to_{date_finish}_{base}_times_{flag}_{target_dimension}.csv', 'a') as f: #appending lines so not overwriting the file\n",
    "        writer = csv.writer(f, delimiter='\\n')\n",
    "        writer.writerow(all_time_window_sieved_times_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def main(date_start, date_finish, target_dimension, time_increment, time_window, flag, home_dir, bases):''' \n",
    "    \n",
    "date_start = '1996-01-01'\n",
    "date_finish = '2011-05-01'\n",
    "target_dimension = 128\n",
    "time_window = 6\n",
    "flag = 'subsample'\n",
    "home_dir = '/home/carl/Documents/'\n",
    "bases = 'EIT195, MDI_96m, LASCO_C2, LASCO_C3, EIT171, EIT304, EIT284' #or a subset of these products\n",
    "\n",
    "date_time_pre_start = date_start + '-0000'\n",
    "date_time_start= parser.parse(date_time_pre_start)\n",
    "print('date_time_start:', date_time_start)\n",
    "\n",
    "date_time_pre_end = date_finish + '-2359'\n",
    "date_time_end = parser.parse(date_time_pre_end)\n",
    "print('date_time_end:', date_time_end)\n",
    "\n",
    "target_dimension = int(target_dimension)\n",
    "print('target_dimension:', target_dimension)\n",
    "\n",
    "time_increment = 60\n",
    "time_window = float(time_window)\n",
    "\n",
    "flag = str(flag) \n",
    "print('flag:', flag)\n",
    "\n",
    "home_dir = str(home_dir)\n",
    "print('home_dir:', home_dir)\n",
    "\n",
    "url_prefix = 'https://seal.nascom.nasa.gov/'\n",
    "print('url_prefix:', url_prefix)\n",
    "\n",
    "look_ahead = int(np.ceil(time_window*60/10)) #should sufficiently cover all 7 products based on their cadence.\n",
    "print('look_ahead:', look_ahead)\n",
    "\n",
    "diff_start_finish_total_sec = (date_time_end - date_time_start).total_seconds()\n",
    "print('diff_start_finish_total_sec:', diff_start_finish_total_sec)\n",
    "\n",
    "total_sec = timedelta(days = time_increment).total_seconds()\n",
    "print('total_sec:', total_sec)\n",
    "\n",
    "num_loops = np.ceil(diff_start_finish_total_sec/total_sec) + 1 #num_loops would be equal to 94 + 1 for 19960101-0000' - '20110501-0000'; discete number of loops so go over rather than under\n",
    "print('num_loops:', num_loops)\n",
    "\n",
    "base_list = bases.split(',')\n",
    "for base in tqdm(base_list):\n",
    "    \n",
    "    start_process_time = process_time() #initialize clock per product type \n",
    "    \n",
    "    base = base.strip(' ')\n",
    "    holes_list = []\n",
    "    unreadable_file_ids_product_list_global = []\n",
    "    \n",
    "    print(f'***{base}***')\n",
    "    base_dir = home_dir + base\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)    \n",
    "\n",
    "    time_range = TimeRange(date_time_start, timedelta(days = time_increment)) #time_range re-initialized here\n",
    "    #print('time_range:', time_range)\n",
    "\n",
    "    prev_time, time_range_modified = prev_time_resumer(home_dir, base, time_range, date_time_end)\n",
    "    for t_value in tqdm(np.arange(num_loops)): #main workhorse loop\n",
    "        print('t_value:', t_value)\n",
    "        print('prev_time:', prev_time)\n",
    "        \n",
    "        if time_range_modified.end > date_time_end:\n",
    "            time_range_modified = TimeRange(time_range_modified.start, date_time_end)\n",
    "            \n",
    "        product_results = product_search(base,time_range_modified,date_time_start)\n",
    "        product_results_number = product_results.file_num\n",
    "        if product_results_number != 0:\n",
    "            ind = index_of_sizes(base,product_results)\n",
    "            all_size_sieved_times_pre, fetch_indices_product_orig = fetch_indices(base,ind,product_results,time_window,look_ahead,prev_time)          \n",
    "            \n",
    "            if len(fetch_indices_product_orig) != 0:\n",
    "                \n",
    "                all_time_window_sieved_times_product_times_modified, holes_product_list, unreadable_file_ids_product_list_local = product_distiller(fetch_indices_product_orig, base, all_size_sieved_times_pre, ind, product_results, look_ahead, time_window, url_prefix, flag, target_dimension, home_dir)\n",
    "                \n",
    "                if holes_product_list:\n",
    "                    holes_list.append(holes_product_list)\n",
    "\n",
    "                if unreadable_file_ids_product_list_local:\n",
    "                    unreadable_file_ids_product_list_global.append(unreadable_file_ids_product_list_local)\n",
    "\n",
    "                all_time_window_sieved_times_sorted = np.unique(all_time_window_sieved_times_product_times_modified)\n",
    "\n",
    "                print(f'{base} np.unique(all_size_sieved_times_pre):', np.unique(all_size_sieved_times_pre), len(np.unique(all_size_sieved_times_pre)))\n",
    "                print(f'{base} list(all_time_window_sieved_times_sorted):', list(all_time_window_sieved_times_sorted), len(all_time_window_sieved_times_sorted))\n",
    "\n",
    "                prev_time = [] #reset to empty list\n",
    "                if len(all_time_window_sieved_times_sorted) != 0:\n",
    "                    prev_time.append(all_time_window_sieved_times_sorted[-1])\n",
    "\n",
    "                csv_writer(base,home_dir,date_start,date_finish,flag,target_dimension, all_time_window_sieved_times_sorted)\n",
    "\n",
    "            else:\n",
    "                holes_list = []\n",
    "                unreadable_file_ids_product_list_global = []\n",
    "        \n",
    "        time_range_modified.next() #Sunpy iterator to go for the next 2 months #also have time_range_modified.previous() to go back.    \n",
    "        #print('time_range_modified next:', time_range_modified)\n",
    "        \n",
    "    print(f'{base} holes_list', holes_list)\n",
    "    print(f'{base} unreadable_file_ids_product_list_global:', unreadable_file_ids_product_list_global)\n",
    "\n",
    "    data_cuber(home_dir, base, date_start, date_finish, flag, target_dimension)\n",
    "        \n",
    "    end_process_time = process_time()\n",
    "    time_of_process = end_process_time - start_process_time\n",
    "    print(f'{base} time of process in seconds:', time_of_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
